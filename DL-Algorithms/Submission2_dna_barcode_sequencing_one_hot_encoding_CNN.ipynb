{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Submission2_dna_barcode_sequencing_one_hot_encoding_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZCK9JQIyxxv"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from google.colab import files\n",
        "from numpy import argmax\n",
        "from numpy import array\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07MWJhIhzDBm"
      },
      "source": [
        "def random_shuffle(filename_features, filename_labels):\n",
        "  one_hot_encoded_seqs_array = clean_seq(filename_features)\n",
        "  labels_df = pd.read_csv(filename_labels)\n",
        "  labels = labels_df['labels']\n",
        "\n",
        "  frames = [pd.DataFrame(labels), pd.DataFrame(one_hot_encoded_seqs_array)]\n",
        "  output_data= np.concatenate(frames, axis=1)\n",
        "  np.random.shuffle(output_data)\n",
        "  one_hot_encoded_seqs_array_shuffled = output_data[:,1:]\n",
        "  labels_shuffled = output_data[:,0]\n",
        "  \n",
        "  return one_hot_encoded_seqs_array_shuffled, labels_shuffled\n",
        "\n",
        "def clean_seq(filename):\n",
        "    seq_df = pd.read_csv(filename, engine='python')\n",
        "    sequences = seq_df['dna'].str.replace('[^ACGT]','N')\n",
        "    for i in range(len(sequences)):\n",
        "        sequences[i] = sequences[i].ljust(1058,'N')\n",
        "    #print(sequences[0])\n",
        "    \n",
        "    sequences_array = np.array(sequences)\n",
        "    #print(pd.DataFrame(sequences_array))\n",
        "    seq_list_padded = []\n",
        "    for i in range(len(sequences_array)):\n",
        "        list_seq = list(sequences_array[i])\n",
        "        del list_seq[750:]\n",
        "        seq_list_padded.append(list_seq)\n",
        "    print(len(list_seq))\n",
        "    replace_map = {'A': 1, 'C': 2, 'G': 3, 'T': 4, 'N': 5}\n",
        "    integer_encoded = []\n",
        "    for i in range(len(seq_list_padded)):\n",
        "        C = (pd.Series(seq_list_padded[i])).map(replace_map) #convert the list to a pandas series temporarily before mapping\n",
        "        integer_encoded.append(list(C))\n",
        "    data = np.array([1, 2, 3, 4, 5])\n",
        "    data = data.reshape(-1, 1) \n",
        "    integer_encoded = np.array(integer_encoded).astype(int)\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    onehot_encoder.fit(data)\n",
        "    one_hot_encoded_seqs = []\n",
        "    for i in range(len(integer_encoded)):\n",
        "        integer_encoded_tmp = integer_encoded[i].reshape(len(integer_encoded[i]), 1)\n",
        "        onehot_encoded = np.array(onehot_encoder.transform(integer_encoded_tmp)).flatten()\n",
        "        one_hot_encoded_seqs.append(onehot_encoded)\n",
        "    one_hot_encoded_seqs_array = np.array(one_hot_encoded_seqs)\n",
        "    print(one_hot_encoded_seqs_array.shape)\n",
        "    \n",
        "    return one_hot_encoded_seqs_array\n",
        "\n",
        "def clean_seq_reshape(one_hot_encoded_seqs_array):\n",
        "    one_hot_encoded_seqs_array = one_hot_encoded_seqs_array.reshape(len(one_hot_encoded_seqs_array), 75, 50)\n",
        "    one_hot_encoded_seqs_array = np.expand_dims(one_hot_encoded_seqs_array, axis=3)\n",
        "\n",
        "    return one_hot_encoded_seqs_array\n",
        "\n",
        "def load_n_encode_labels(labels):\n",
        "    softmax_layer = len(set(labels))\n",
        "    le = LabelEncoder()\n",
        "    le.fit(labels)\n",
        "    label_seq = le.transform(labels)\n",
        "    #label_seq = label_seq.reshape(len(label_seq), 1)\n",
        "    \n",
        "    return label_seq, le, softmax_layer\n",
        "\n",
        "def decode_labels(encoded_predict_labels, le):\n",
        "    test_predictions = le.inverse_transform(encoded_predict_labels)\n",
        "    \n",
        "    return test_predictions\n",
        "\n",
        "def generate_class_weights(class_series):\n",
        "  # Compute class weights with sklearn method\n",
        "  class_labels = np.unique(class_series)\n",
        "  class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
        "\n",
        "  return dict(zip(class_labels, class_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "bSaPaF4UzDEm",
        "outputId": "01a0e961-4bac-4a25-e37a-6286c758dd3c"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0abad5a7-7d79-4c29-b5ec-31a3206f2141\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0abad5a7-7d79-4c29-b5ec-31a3206f2141\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_features.csv to test_features.csv\n",
            "Saving train_features.csv to train_features.csv\n",
            "Saving train_labels.csv to train_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L40vfbizwq0",
        "outputId": "0c8c4110-b9a6-451e-f3c9-5dbf7a73934c"
      },
      "source": [
        "filename_features = 'train_features.csv'\n",
        "filename_labels = 'train_labels.csv'\n",
        "one_hot_encoded_seqs_array_shuffled, labels_shuffled = random_shuffle(filename_features, filename_labels)\n",
        "one_hot_encoded_seqs_array = clean_seq_reshape(one_hot_encoded_seqs_array_shuffled)\n",
        "label_seq, le, softmax_layer = load_n_encode_labels(labels_shuffled)\n",
        "input_shape = (75, 50, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750\n",
            "(12906, 3750)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSzV9oDKEVqD"
      },
      "source": [
        "# Create a callback that saves the model's weights\n",
        "#https://jonathan-hui.medium.com/tensorflow-save-restore-model-75a1e6d3b9a6\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
        "\n",
        "callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"mymodel_{epoch}\", save_weights_only=True,\n",
        "        save_best_only = True, monitor = \"val_loss\", verbose =1,)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnRgInb0dZJR",
        "outputId": "c696cb10-780d-467d-92e0-605dbef58bf9"
      },
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(one_hot_encoded_seqs_array, label_seq, test_size=0.2)\n",
        "class_weights = generate_class_weights(y_train)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_validation.shape)\n",
        "print(y_validation.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10324, 75, 50, 1)\n",
            "(10324,)\n",
            "(2582, 75, 50, 1)\n",
            "(2582,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBQIY-fSaNcz",
        "outputId": "3f2a5051-3208-4051-aa56-e438ea0e3819"
      },
      "source": [
        "print(len(y_train))\n",
        "print(len(set(y_train)))\n",
        "print(len(class_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10324\n",
            "1202\n",
            "1202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE0Fuog-kc-V",
        "outputId": "23a2f2c0-3d98-4a23-e2e5-cd67373c6b9d"
      },
      "source": [
        "print(len(labels_shuffled))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ6caE_MnWc4",
        "outputId": "f38096cd-ca5b-417d-930d-5e8811d98b04"
      },
      "source": [
        "X_train = one_hot_encoded_seqs_array\n",
        "y_train = label_seq\n",
        "class_weights = generate_class_weights(y_train)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(len(y_train))\n",
        "print(len(set(y_train)))\n",
        "print(len(class_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12906, 65, 50, 1)\n",
            "(12906,)\n",
            "12906\n",
            "1202\n",
            "1202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uq0tCujodMtA",
        "outputId": "6584ed52-cc6e-4dc8-fdfc-4fb018cf2016"
      },
      "source": [
        "#seq len 750 per species\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (2, 2), activation='relu', input_shape=input_shape),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(32, (2, 2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(), \n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(softmax_layer, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer=optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss='sparse_categorical_crossentropy'\n",
        "model.compile(loss=loss,optimizer=optimizer,metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=64, epochs=num_epochs, validation_data=(X_validation, y_validation), callbacks=callbacks, verbose=2, class_weight = class_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 64, 49, 16)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 32, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 32, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 31, 23, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 15, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 15, 11, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 15, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 10, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 7, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 7, 5, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 7, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2240)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1000)              2241000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1202)              1203202   \n",
            "=================================================================\n",
            "Total params: 3,459,066\n",
            "Trainable params: 3,456,842\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "172/172 - 4s - loss: 4.1871 - accuracy: 0.4808 - val_loss: 6.8755 - val_accuracy: 0.0207\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.28103\n",
            "Epoch 2/100\n",
            "172/172 - 2s - loss: 0.5122 - accuracy: 0.9232 - val_loss: 4.6409 - val_accuracy: 0.1885\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.28103\n",
            "Epoch 3/100\n",
            "172/172 - 2s - loss: 0.2094 - accuracy: 0.9584 - val_loss: 0.4815 - val_accuracy: 0.9272\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.28103\n",
            "Epoch 4/100\n",
            "172/172 - 2s - loss: 0.1025 - accuracy: 0.9773 - val_loss: 0.2786 - val_accuracy: 0.9561\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.28103 to 0.27858, saving model to mymodel_4\n",
            "Epoch 5/100\n",
            "172/172 - 2s - loss: 0.0700 - accuracy: 0.9864 - val_loss: 0.2665 - val_accuracy: 0.9592\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.27858 to 0.26654, saving model to mymodel_5\n",
            "Epoch 6/100\n",
            "172/172 - 2s - loss: 0.0632 - accuracy: 0.9892 - val_loss: 0.2547 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.26654 to 0.25466, saving model to mymodel_6\n",
            "Epoch 7/100\n",
            "172/172 - 2s - loss: 0.0617 - accuracy: 0.9895 - val_loss: 0.2848 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.25466\n",
            "Epoch 8/100\n",
            "172/172 - 2s - loss: 0.0574 - accuracy: 0.9908 - val_loss: 0.2655 - val_accuracy: 0.9623\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.25466\n",
            "Epoch 9/100\n",
            "172/172 - 2s - loss: 0.0478 - accuracy: 0.9910 - val_loss: 0.2619 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.25466\n",
            "Epoch 10/100\n",
            "172/172 - 2s - loss: 0.0579 - accuracy: 0.9910 - val_loss: 0.3466 - val_accuracy: 0.9592\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.25466\n",
            "Epoch 11/100\n",
            "172/172 - 2s - loss: 0.0490 - accuracy: 0.9908 - val_loss: 0.2727 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.25466\n",
            "Epoch 12/100\n",
            "172/172 - 2s - loss: 0.0570 - accuracy: 0.9909 - val_loss: 0.2996 - val_accuracy: 0.9623\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.25466\n",
            "Epoch 13/100\n",
            "172/172 - 2s - loss: 0.0432 - accuracy: 0.9940 - val_loss: 0.3173 - val_accuracy: 0.9597\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.25466\n",
            "Epoch 14/100\n",
            "172/172 - 2s - loss: 0.0444 - accuracy: 0.9933 - val_loss: 0.3021 - val_accuracy: 0.9618\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.25466\n",
            "Epoch 15/100\n",
            "172/172 - 2s - loss: 0.0594 - accuracy: 0.9914 - val_loss: 0.2958 - val_accuracy: 0.9618\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.25466\n",
            "Epoch 16/100\n",
            "172/172 - 2s - loss: 0.0457 - accuracy: 0.9912 - val_loss: 0.3248 - val_accuracy: 0.9618\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.25466\n",
            "Epoch 17/100\n",
            "172/172 - 2s - loss: 0.0507 - accuracy: 0.9908 - val_loss: 0.3635 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.25466\n",
            "Epoch 18/100\n",
            "172/172 - 2s - loss: 0.0652 - accuracy: 0.9894 - val_loss: 0.3614 - val_accuracy: 0.9607\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.25466\n",
            "Epoch 19/100\n",
            "172/172 - 2s - loss: 0.0539 - accuracy: 0.9898 - val_loss: 0.3588 - val_accuracy: 0.9556\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.25466\n",
            "Epoch 20/100\n",
            "172/172 - 2s - loss: 0.0329 - accuracy: 0.9933 - val_loss: 0.3857 - val_accuracy: 0.9571\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.25466\n",
            "Epoch 21/100\n",
            "172/172 - 2s - loss: 0.0403 - accuracy: 0.9924 - val_loss: 0.3445 - val_accuracy: 0.9592\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.25466\n",
            "Epoch 22/100\n",
            "172/172 - 2s - loss: 0.0494 - accuracy: 0.9922 - val_loss: 0.3266 - val_accuracy: 0.9623\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.25466\n",
            "Epoch 23/100\n",
            "172/172 - 2s - loss: 0.0381 - accuracy: 0.9919 - val_loss: 0.3266 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.25466\n",
            "Epoch 24/100\n",
            "172/172 - 2s - loss: 0.0388 - accuracy: 0.9937 - val_loss: 0.3339 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.25466\n",
            "Epoch 25/100\n",
            "172/172 - 2s - loss: 0.0322 - accuracy: 0.9941 - val_loss: 0.3210 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.25466\n",
            "Epoch 26/100\n",
            "172/172 - 2s - loss: 0.0471 - accuracy: 0.9935 - val_loss: 0.3480 - val_accuracy: 0.9602\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.25466\n",
            "Epoch 27/100\n",
            "172/172 - 2s - loss: 0.0385 - accuracy: 0.9923 - val_loss: 0.3630 - val_accuracy: 0.9587\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.25466\n",
            "Epoch 28/100\n",
            "172/172 - 2s - loss: 0.0582 - accuracy: 0.9900 - val_loss: 0.3602 - val_accuracy: 0.9607\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.25466\n",
            "Epoch 29/100\n",
            "172/172 - 2s - loss: 0.0421 - accuracy: 0.9927 - val_loss: 0.3579 - val_accuracy: 0.9618\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.25466\n",
            "Epoch 30/100\n",
            "172/172 - 2s - loss: 0.0282 - accuracy: 0.9936 - val_loss: 0.3465 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.25466\n",
            "Epoch 31/100\n",
            "172/172 - 2s - loss: 0.0354 - accuracy: 0.9943 - val_loss: 0.3669 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.25466\n",
            "Epoch 32/100\n",
            "172/172 - 2s - loss: 0.0373 - accuracy: 0.9929 - val_loss: 0.3583 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.25466\n",
            "Epoch 33/100\n",
            "172/172 - 2s - loss: 0.0401 - accuracy: 0.9933 - val_loss: 0.3471 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.25466\n",
            "Epoch 34/100\n",
            "172/172 - 2s - loss: 0.0257 - accuracy: 0.9943 - val_loss: 0.3671 - val_accuracy: 0.9638\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.25466\n",
            "Epoch 35/100\n",
            "172/172 - 2s - loss: 0.0282 - accuracy: 0.9945 - val_loss: 0.3565 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.25466\n",
            "Epoch 36/100\n",
            "172/172 - 2s - loss: 0.0356 - accuracy: 0.9937 - val_loss: 0.3935 - val_accuracy: 0.9623\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.25466\n",
            "Epoch 37/100\n",
            "172/172 - 2s - loss: 0.0300 - accuracy: 0.9947 - val_loss: 0.3673 - val_accuracy: 0.9592\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.25466\n",
            "Epoch 38/100\n",
            "172/172 - 2s - loss: 0.0327 - accuracy: 0.9935 - val_loss: 0.3597 - val_accuracy: 0.9618\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.25466\n",
            "Epoch 39/100\n",
            "172/172 - 2s - loss: 0.0339 - accuracy: 0.9927 - val_loss: 0.3795 - val_accuracy: 0.9618\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.25466\n",
            "Epoch 40/100\n",
            "172/172 - 2s - loss: 0.0265 - accuracy: 0.9950 - val_loss: 0.3397 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.25466\n",
            "Epoch 41/100\n",
            "172/172 - 2s - loss: 0.0273 - accuracy: 0.9953 - val_loss: 0.3952 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.25466\n",
            "Epoch 42/100\n",
            "172/172 - 2s - loss: 0.0301 - accuracy: 0.9947 - val_loss: 0.3470 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.25466\n",
            "Epoch 43/100\n",
            "172/172 - 2s - loss: 0.0234 - accuracy: 0.9951 - val_loss: 0.3305 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.25466\n",
            "Epoch 44/100\n",
            "172/172 - 2s - loss: 0.0378 - accuracy: 0.9944 - val_loss: 0.4403 - val_accuracy: 0.9602\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.25466\n",
            "Epoch 45/100\n",
            "172/172 - 2s - loss: 0.0294 - accuracy: 0.9940 - val_loss: 0.3851 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.25466\n",
            "Epoch 46/100\n",
            "172/172 - 2s - loss: 0.0301 - accuracy: 0.9935 - val_loss: 0.3850 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.25466\n",
            "Epoch 47/100\n",
            "172/172 - 2s - loss: 0.0211 - accuracy: 0.9957 - val_loss: 0.3690 - val_accuracy: 0.9669\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.25466\n",
            "Epoch 48/100\n",
            "172/172 - 2s - loss: 0.0391 - accuracy: 0.9933 - val_loss: 0.4279 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.25466\n",
            "Epoch 49/100\n",
            "172/172 - 2s - loss: 0.0306 - accuracy: 0.9940 - val_loss: 0.4146 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.25466\n",
            "Epoch 50/100\n",
            "172/172 - 2s - loss: 0.0249 - accuracy: 0.9945 - val_loss: 0.3843 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.25466\n",
            "Epoch 51/100\n",
            "172/172 - 2s - loss: 0.0191 - accuracy: 0.9955 - val_loss: 0.3972 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.25466\n",
            "Epoch 52/100\n",
            "172/172 - 2s - loss: 0.0230 - accuracy: 0.9953 - val_loss: 0.4351 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.25466\n",
            "Epoch 53/100\n",
            "172/172 - 2s - loss: 0.0304 - accuracy: 0.9933 - val_loss: 0.4269 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.25466\n",
            "Epoch 54/100\n",
            "172/172 - 2s - loss: 0.0243 - accuracy: 0.9951 - val_loss: 0.4355 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.25466\n",
            "Epoch 55/100\n",
            "172/172 - 2s - loss: 0.0276 - accuracy: 0.9951 - val_loss: 0.4261 - val_accuracy: 0.9638\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.25466\n",
            "Epoch 56/100\n",
            "172/172 - 2s - loss: 0.0185 - accuracy: 0.9959 - val_loss: 0.4050 - val_accuracy: 0.9623\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.25466\n",
            "Epoch 57/100\n",
            "172/172 - 2s - loss: 0.0206 - accuracy: 0.9968 - val_loss: 0.3972 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.25466\n",
            "Epoch 58/100\n",
            "172/172 - 2s - loss: 0.0258 - accuracy: 0.9953 - val_loss: 0.3955 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.25466\n",
            "Epoch 59/100\n",
            "172/172 - 2s - loss: 0.0212 - accuracy: 0.9964 - val_loss: 0.4372 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.25466\n",
            "Epoch 60/100\n",
            "172/172 - 2s - loss: 0.0265 - accuracy: 0.9956 - val_loss: 0.4109 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.25466\n",
            "Epoch 61/100\n",
            "172/172 - 2s - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.4046 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.25466\n",
            "Epoch 62/100\n",
            "172/172 - 2s - loss: 0.0243 - accuracy: 0.9947 - val_loss: 0.3932 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.25466\n",
            "Epoch 63/100\n",
            "172/172 - 2s - loss: 0.0097 - accuracy: 0.9975 - val_loss: 0.3947 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.25466\n",
            "Epoch 64/100\n",
            "172/172 - 2s - loss: 0.0196 - accuracy: 0.9967 - val_loss: 0.3858 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.25466\n",
            "Epoch 65/100\n",
            "172/172 - 2s - loss: 0.0139 - accuracy: 0.9968 - val_loss: 0.3859 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.25466\n",
            "Epoch 66/100\n",
            "172/172 - 2s - loss: 0.0190 - accuracy: 0.9956 - val_loss: 0.4235 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.25466\n",
            "Epoch 67/100\n",
            "172/172 - 2s - loss: 0.0224 - accuracy: 0.9964 - val_loss: 0.4197 - val_accuracy: 0.9638\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.25466\n",
            "Epoch 68/100\n",
            "172/172 - 2s - loss: 0.0140 - accuracy: 0.9963 - val_loss: 0.4967 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.25466\n",
            "Epoch 69/100\n",
            "172/172 - 2s - loss: 0.0288 - accuracy: 0.9957 - val_loss: 0.4870 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.25466\n",
            "Epoch 70/100\n",
            "172/172 - 2s - loss: 0.0364 - accuracy: 0.9951 - val_loss: 0.4401 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.25466\n",
            "Epoch 71/100\n",
            "172/172 - 2s - loss: 0.0187 - accuracy: 0.9955 - val_loss: 0.4224 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.25466\n",
            "Epoch 72/100\n",
            "172/172 - 2s - loss: 0.0218 - accuracy: 0.9961 - val_loss: 0.4304 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.25466\n",
            "Epoch 73/100\n",
            "172/172 - 2s - loss: 0.0162 - accuracy: 0.9967 - val_loss: 0.4359 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.25466\n",
            "Epoch 74/100\n",
            "172/172 - 2s - loss: 0.0174 - accuracy: 0.9968 - val_loss: 0.4055 - val_accuracy: 0.9669\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.25466\n",
            "Epoch 75/100\n",
            "172/172 - 2s - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.4228 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.25466\n",
            "Epoch 76/100\n",
            "172/172 - 2s - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.4337 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.25466\n",
            "Epoch 77/100\n",
            "172/172 - 2s - loss: 0.0197 - accuracy: 0.9977 - val_loss: 0.4196 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.25466\n",
            "Epoch 78/100\n",
            "172/172 - 2s - loss: 0.0224 - accuracy: 0.9956 - val_loss: 0.4600 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.25466\n",
            "Epoch 79/100\n",
            "172/172 - 2s - loss: 0.0229 - accuracy: 0.9950 - val_loss: 0.4448 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.25466\n",
            "Epoch 80/100\n",
            "172/172 - 2s - loss: 0.0191 - accuracy: 0.9963 - val_loss: 0.4235 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.25466\n",
            "Epoch 81/100\n",
            "172/172 - 2s - loss: 0.0190 - accuracy: 0.9964 - val_loss: 0.4426 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.25466\n",
            "Epoch 82/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-64b0b83965b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYIhqPkrb0pc",
        "outputId": "94bbada3-9c15-429f-a509-18348b740f54"
      },
      "source": [
        "#seq len 650 per species\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (2, 2), activation='relu', input_shape=input_shape),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(32, (2, 2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(), \n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(softmax_layer, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer=optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss='sparse_categorical_crossentropy'\n",
        "model.compile(loss=loss,optimizer=optimizer,metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=64, epochs=num_epochs, validation_data=(X_validation, y_validation), callbacks=callbacks, verbose=2, class_weight = class_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 64, 49, 16)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 32, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 32, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 31, 23, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 15, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 15, 11, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 15, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 14, 10, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 7, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 7, 5, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 7, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2240)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1000)              2241000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1202)              1203202   \n",
            "=================================================================\n",
            "Total params: 3,459,066\n",
            "Trainable params: 3,456,842\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "162/162 - 3s - loss: 7.2813 - accuracy: 0.0034 - val_loss: 7.3038 - val_accuracy: 3.8730e-04\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.29020\n",
            "Epoch 2/100\n",
            "162/162 - 2s - loss: 6.3448 - accuracy: 0.0511 - val_loss: 7.0447 - val_accuracy: 0.0019\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.29020\n",
            "Epoch 3/100\n",
            "162/162 - 2s - loss: 5.3322 - accuracy: 0.2199 - val_loss: 5.3384 - val_accuracy: 0.1987\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.29020\n",
            "Epoch 4/100\n",
            "162/162 - 2s - loss: 4.1827 - accuracy: 0.4940 - val_loss: 2.8303 - val_accuracy: 0.7258\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.29020\n",
            "Epoch 5/100\n",
            "162/162 - 2s - loss: 3.0643 - accuracy: 0.7002 - val_loss: 1.5553 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.29020\n",
            "Epoch 6/100\n",
            "162/162 - 2s - loss: 2.1449 - accuracy: 0.8121 - val_loss: 1.0481 - val_accuracy: 0.8730\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.29020\n",
            "Epoch 7/100\n",
            "162/162 - 2s - loss: 1.4667 - accuracy: 0.8692 - val_loss: 0.7893 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.29020\n",
            "Epoch 8/100\n",
            "162/162 - 2s - loss: 1.0460 - accuracy: 0.9004 - val_loss: 0.6443 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.29020\n",
            "Epoch 9/100\n",
            "162/162 - 2s - loss: 0.7761 - accuracy: 0.9219 - val_loss: 0.5519 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.29020\n",
            "Epoch 10/100\n",
            "162/162 - 2s - loss: 0.6025 - accuracy: 0.9347 - val_loss: 0.4919 - val_accuracy: 0.9249\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.29020\n",
            "Epoch 11/100\n",
            "162/162 - 2s - loss: 0.4746 - accuracy: 0.9425 - val_loss: 0.4472 - val_accuracy: 0.9303\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.29020\n",
            "Epoch 12/100\n",
            "162/162 - 2s - loss: 0.3884 - accuracy: 0.9522 - val_loss: 0.4131 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.29020\n",
            "Epoch 13/100\n",
            "162/162 - 2s - loss: 0.3213 - accuracy: 0.9586 - val_loss: 0.3848 - val_accuracy: 0.9388\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.29020\n",
            "Epoch 14/100\n",
            "162/162 - 2s - loss: 0.2611 - accuracy: 0.9644 - val_loss: 0.3651 - val_accuracy: 0.9419\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.29020\n",
            "Epoch 15/100\n",
            "162/162 - 2s - loss: 0.2248 - accuracy: 0.9694 - val_loss: 0.3516 - val_accuracy: 0.9419\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.29020\n",
            "Epoch 16/100\n",
            "162/162 - 2s - loss: 0.1900 - accuracy: 0.9757 - val_loss: 0.3387 - val_accuracy: 0.9446\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.29020\n",
            "Epoch 17/100\n",
            "162/162 - 2s - loss: 0.1630 - accuracy: 0.9774 - val_loss: 0.3306 - val_accuracy: 0.9450\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.29020\n",
            "Epoch 18/100\n",
            "162/162 - 2s - loss: 0.1427 - accuracy: 0.9800 - val_loss: 0.3194 - val_accuracy: 0.9481\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.29020\n",
            "Epoch 19/100\n",
            "162/162 - 2s - loss: 0.1218 - accuracy: 0.9824 - val_loss: 0.3144 - val_accuracy: 0.9493\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.29020\n",
            "Epoch 20/100\n",
            "162/162 - 2s - loss: 0.1036 - accuracy: 0.9847 - val_loss: 0.3078 - val_accuracy: 0.9504\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.29020\n",
            "Epoch 21/100\n",
            "162/162 - 2s - loss: 0.0876 - accuracy: 0.9872 - val_loss: 0.3046 - val_accuracy: 0.9512\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.29020\n",
            "Epoch 22/100\n",
            "162/162 - 2s - loss: 0.0794 - accuracy: 0.9887 - val_loss: 0.3000 - val_accuracy: 0.9527\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.29020\n",
            "Epoch 23/100\n",
            "162/162 - 2s - loss: 0.0670 - accuracy: 0.9894 - val_loss: 0.2959 - val_accuracy: 0.9551\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.29020\n",
            "Epoch 24/100\n",
            "162/162 - 2s - loss: 0.0563 - accuracy: 0.9924 - val_loss: 0.2916 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.29020\n",
            "Epoch 25/100\n",
            "162/162 - 2s - loss: 0.0518 - accuracy: 0.9928 - val_loss: 0.2896 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.29020 to 0.28961, saving model to mymodel_25\n",
            "Epoch 26/100\n",
            "162/162 - 2s - loss: 0.0453 - accuracy: 0.9937 - val_loss: 0.2896 - val_accuracy: 0.9547\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.28961\n",
            "Epoch 27/100\n",
            "162/162 - 2s - loss: 0.0420 - accuracy: 0.9943 - val_loss: 0.2867 - val_accuracy: 0.9562\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.28961 to 0.28673, saving model to mymodel_27\n",
            "Epoch 28/100\n",
            "162/162 - 2s - loss: 0.0388 - accuracy: 0.9946 - val_loss: 0.2882 - val_accuracy: 0.9558\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.28673\n",
            "Epoch 29/100\n",
            "162/162 - 2s - loss: 0.0359 - accuracy: 0.9943 - val_loss: 0.2832 - val_accuracy: 0.9589\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.28673 to 0.28318, saving model to mymodel_29\n",
            "Epoch 30/100\n",
            "162/162 - 2s - loss: 0.0313 - accuracy: 0.9946 - val_loss: 0.2870 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.28318\n",
            "Epoch 31/100\n",
            "162/162 - 2s - loss: 0.0306 - accuracy: 0.9954 - val_loss: 0.2831 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.28318 to 0.28312, saving model to mymodel_31\n",
            "Epoch 32/100\n",
            "162/162 - 2s - loss: 0.0284 - accuracy: 0.9951 - val_loss: 0.2842 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.28312\n",
            "Epoch 33/100\n",
            "162/162 - 2s - loss: 0.0251 - accuracy: 0.9962 - val_loss: 0.2887 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.28312\n",
            "Epoch 34/100\n",
            "162/162 - 2s - loss: 0.0230 - accuracy: 0.9963 - val_loss: 0.2848 - val_accuracy: 0.9597\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.28312\n",
            "Epoch 35/100\n",
            "162/162 - 2s - loss: 0.0212 - accuracy: 0.9967 - val_loss: 0.2810 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.28312 to 0.28103, saving model to mymodel_35\n",
            "Epoch 36/100\n",
            "162/162 - 2s - loss: 0.0205 - accuracy: 0.9971 - val_loss: 0.2848 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.28103\n",
            "Epoch 37/100\n",
            "162/162 - 2s - loss: 0.0191 - accuracy: 0.9959 - val_loss: 0.2869 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.28103\n",
            "Epoch 38/100\n",
            "162/162 - 2s - loss: 0.0178 - accuracy: 0.9965 - val_loss: 0.2830 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.28103\n",
            "Epoch 39/100\n",
            "162/162 - 2s - loss: 0.0165 - accuracy: 0.9968 - val_loss: 0.2858 - val_accuracy: 0.9589\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.28103\n",
            "Epoch 40/100\n",
            "162/162 - 2s - loss: 0.0161 - accuracy: 0.9966 - val_loss: 0.2940 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.28103\n",
            "Epoch 41/100\n",
            "162/162 - 2s - loss: 0.0142 - accuracy: 0.9973 - val_loss: 0.2904 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.28103\n",
            "Epoch 42/100\n",
            "162/162 - 2s - loss: 0.0150 - accuracy: 0.9964 - val_loss: 0.2882 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.28103\n",
            "Epoch 43/100\n",
            "162/162 - 2s - loss: 0.0138 - accuracy: 0.9968 - val_loss: 0.2887 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.28103\n",
            "Epoch 44/100\n",
            "162/162 - 2s - loss: 0.0121 - accuracy: 0.9976 - val_loss: 0.2970 - val_accuracy: 0.9562\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.28103\n",
            "Epoch 45/100\n",
            "162/162 - 2s - loss: 0.0126 - accuracy: 0.9976 - val_loss: 0.2870 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.28103\n",
            "Epoch 46/100\n",
            "162/162 - 2s - loss: 0.0127 - accuracy: 0.9970 - val_loss: 0.2885 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.28103\n",
            "Epoch 47/100\n",
            "162/162 - 2s - loss: 0.0109 - accuracy: 0.9977 - val_loss: 0.2906 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.28103\n",
            "Epoch 48/100\n",
            "162/162 - 2s - loss: 0.0094 - accuracy: 0.9978 - val_loss: 0.2922 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.28103\n",
            "Epoch 49/100\n",
            "162/162 - 2s - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.2900 - val_accuracy: 0.9566\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.28103\n",
            "Epoch 50/100\n",
            "162/162 - 2s - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.2916 - val_accuracy: 0.9562\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.28103\n",
            "Epoch 51/100\n",
            "162/162 - 2s - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.2898 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.28103\n",
            "Epoch 52/100\n",
            "162/162 - 2s - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.2955 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.28103\n",
            "Epoch 53/100\n",
            "162/162 - 2s - loss: 0.0096 - accuracy: 0.9977 - val_loss: 0.2911 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.28103\n",
            "Epoch 54/100\n",
            "162/162 - 2s - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.2895 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.28103\n",
            "Epoch 55/100\n",
            "162/162 - 2s - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.2989 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.28103\n",
            "Epoch 56/100\n",
            "162/162 - 2s - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.2940 - val_accuracy: 0.9597\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.28103\n",
            "Epoch 57/100\n",
            "162/162 - 2s - loss: 0.0083 - accuracy: 0.9976 - val_loss: 0.2929 - val_accuracy: 0.9597\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.28103\n",
            "Epoch 58/100\n",
            "162/162 - 2s - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.2945 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.28103\n",
            "Epoch 59/100\n",
            "162/162 - 2s - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.2988 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.28103\n",
            "Epoch 60/100\n",
            "162/162 - 2s - loss: 0.0093 - accuracy: 0.9977 - val_loss: 0.2934 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.28103\n",
            "Epoch 61/100\n",
            "162/162 - 2s - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.3043 - val_accuracy: 0.9562\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.28103\n",
            "Epoch 62/100\n",
            "162/162 - 2s - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.2959 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.28103\n",
            "Epoch 63/100\n",
            "162/162 - 2s - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.2935 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.28103\n",
            "Epoch 64/100\n",
            "162/162 - 2s - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.2973 - val_accuracy: 0.9566\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.28103\n",
            "Epoch 65/100\n",
            "162/162 - 2s - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.3010 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.28103\n",
            "Epoch 66/100\n",
            "162/162 - 2s - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.3083 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.28103\n",
            "Epoch 67/100\n",
            "162/162 - 2s - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.3023 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.28103\n",
            "Epoch 68/100\n",
            "162/162 - 2s - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.3029 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.28103\n",
            "Epoch 69/100\n",
            "162/162 - 2s - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.3029 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.28103\n",
            "Epoch 70/100\n",
            "162/162 - 2s - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.3095 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.28103\n",
            "Epoch 71/100\n",
            "162/162 - 2s - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.3035 - val_accuracy: 0.9589\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.28103\n",
            "Epoch 72/100\n",
            "162/162 - 2s - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.3038 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.28103\n",
            "Epoch 73/100\n",
            "162/162 - 2s - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.3107 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.28103\n",
            "Epoch 74/100\n",
            "162/162 - 2s - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.3156 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.28103\n",
            "Epoch 75/100\n",
            "162/162 - 2s - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.3040 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.28103\n",
            "Epoch 76/100\n",
            "162/162 - 2s - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.3163 - val_accuracy: 0.9562\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.28103\n",
            "Epoch 77/100\n",
            "162/162 - 2s - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.3184 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.28103\n",
            "Epoch 78/100\n",
            "162/162 - 2s - loss: 0.0060 - accuracy: 0.9978 - val_loss: 0.3133 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.28103\n",
            "Epoch 79/100\n",
            "162/162 - 2s - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.3227 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.28103\n",
            "Epoch 80/100\n",
            "162/162 - 2s - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.3164 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.28103\n",
            "Epoch 81/100\n",
            "162/162 - 2s - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.3239 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.28103\n",
            "Epoch 82/100\n",
            "162/162 - 2s - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.3130 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.28103\n",
            "Epoch 83/100\n",
            "162/162 - 2s - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.3148 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.28103\n",
            "Epoch 84/100\n",
            "162/162 - 2s - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.3146 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.28103\n",
            "Epoch 85/100\n",
            "162/162 - 2s - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.3194 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.28103\n",
            "Epoch 86/100\n",
            "162/162 - 2s - loss: 0.0027 - accuracy: 0.9986 - val_loss: 0.3192 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.28103\n",
            "Epoch 87/100\n",
            "162/162 - 2s - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.3104 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.28103\n",
            "Epoch 88/100\n",
            "162/162 - 2s - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.3197 - val_accuracy: 0.9593\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.28103\n",
            "Epoch 89/100\n",
            "162/162 - 2s - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.3121 - val_accuracy: 0.9593\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.28103\n",
            "Epoch 90/100\n",
            "162/162 - 2s - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.3218 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.28103\n",
            "Epoch 91/100\n",
            "162/162 - 2s - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.3257 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.28103\n",
            "Epoch 92/100\n",
            "162/162 - 2s - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.3160 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.28103\n",
            "Epoch 93/100\n",
            "162/162 - 2s - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.3206 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.28103\n",
            "Epoch 94/100\n",
            "162/162 - 2s - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.3262 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.28103\n",
            "Epoch 95/100\n",
            "162/162 - 2s - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.3120 - val_accuracy: 0.9593\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.28103\n",
            "Epoch 96/100\n",
            "162/162 - 2s - loss: 0.0042 - accuracy: 0.9984 - val_loss: 0.3169 - val_accuracy: 0.9593\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.28103\n",
            "Epoch 97/100\n",
            "162/162 - 2s - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.3210 - val_accuracy: 0.9589\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.28103\n",
            "Epoch 98/100\n",
            "162/162 - 2s - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.3234 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.28103\n",
            "Epoch 99/100\n",
            "162/162 - 2s - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.3263 - val_accuracy: 0.9589\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.28103\n",
            "Epoch 100/100\n",
            "162/162 - 2s - loss: 0.0023 - accuracy: 0.9989 - val_loss: 0.3225 - val_accuracy: 0.9589\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.28103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "H4JBbyH6klD0",
        "outputId": "11da17ea-06cf-4f18-91c0-75fe5dcf30e0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwcdbnv8c8z3bNP9p0sJECAJIQQMoTNI5HlXFAERUPgIFcjywUFWTwHEI+Ay7nHo+folXPQSzxC5CCCsstFuCxB9BKQRDAJCWiAkJmQZcgkM5nM1stz/6jqSWeYpZOZTidT3/fr1a/uqq6u+lV1dz311K/q9zN3R0REoquo0AUQEZHCUiAQEYk4BQIRkYhTIBARiTgFAhGRiIsXugB7auTIkT558uRCF0NE5ICyfPnyD9x9VFfvHXCBYPLkySxbtqzQxRAROaCY2XvdvadTQyIiEadAICIScQoEIiIRp0AgIhJxCgQiIhGXt0BgZneZ2RYzW9XN+2Zmt5vZWjNbYWbH5qssIiLSvXxmBIuBM3t4/yxgavi4HPhJHssiIiLdyNt9BO7+oplN7mGSc4F7PGgH+2UzG2pm49x9Y77KJIWVSjvJdJpkykmmHRyw4D2z4KWZkUimaUmkaG5P0ZpIdbxOptJdzjftkEqnSaR2n38y7WSaWXcHdyfT6LoBsSLDzCgyI1YULLs4ZpTEYpTEi4JHLHgeP7ScsUPK8r2JepROOy2JFDvbkzS3BdukPZUmkUqTSKZpT6VpT+7aDqlwG3Q8p9KYGfGYUVxURKwofB0LjgcTqfAzKSeR3vU6lXZSHj6Hj47m680yX2H4HQbbsqjIiJkRK9r90Xm5saLg88m0B+uRCp6T4eu0Zx50LDedWXT2ci0YKjLDDIrCsgSvg/ccdvs9BOOC+blD2oP5e/jTjBUZ8bDc8SIjFisiXmRUlMSoKIlTWRKjojQeDscYO7iMeOzAPNteyBvKxgM1WcO14bgPBQIzu5wga2DSpEn7pHB7qqU9xeqNjdTUN9OSSNHSntr9ORHs1NoSadqSKVoTaVoTwR85swMssl07p5gZ5SUxBpXFqSqNU1UWZ1BpnEFlxQytKGbs4DLGDiljzOAyyopjeV8/d6c1kaahJcG25na2NbezvTnR8bw9fG5o2f3R2JII/tzpNAdy1xdm8LEjRnPxiQdzytRRFBVZ7x/KUXsyzXNrNrNmYyPbs7bd9uZg+zW2JmluT9Lcnuq3ZUr/G1Qa57gpwznxkBGceOgIpo0bTKyPv5O2ZIq1W5pYs3EHb25s5ONHj+PYScP6qcS7HBB3Frv7ImARQHV1dcF3J01tSf6yeQerNjSworaBVRsa+MvmHR1HKtmKDMqLY5SXxCiNxygrLup4LiuOUVUWfAVpD4740u4dR85bdiR4uy5JU2uSHW1J2pNdHxFnB4YJw8qZMKyC8UPLO16PrCrpOGLqSirt1NQ3s3ZLE2vrmnh7SxN1TW0dO/KGliSNLQnauzkiBygrLmJoeQlDyosZUlHMxOEVzCwvZlBZMSXxIopjRryoiHhs11FWpky7HbXjlMSKKC+JUVYco7w4OPoqLykiXlREV6thWMd84+FRWzwWLKNotyNWC4NuuL0zR5tpsra7054MjqzbUynakmnakmlee28b9/2xhoV3v8qk4RV87oRJnF89kaEVJd1uk96s3dLEA6+u56E/baB+ZztmMDgM9EPKg8eEYeUMKiumqjQ8Ci2NUR4ejZYXxygtLqI4FmQuxWEGUxzbfTvHw6PwWJHh7h1H37sypyBI7/4d7T6PzLbs/N1lZB9Jp7OzB3fSnTKTzNF+KiwH0FHmkvC5OPweM5lF5kg/k0F0LDfrd5M50EiHZUmHGWA67XR8ynbPPiH4j2bmbwQHZEBHFhRkU7u2W+YAb2dbEJx3tgf/0RUbGnj57a08/+YWAAaXxTn+kBEcP2U4c6cMZ/q4wT1mDM3tSVbUNvDnmu2s2djImo07eLuuKciegdJ4EVPHVOUlEFg+eygLTw094e5HdfHencAL7v7LcPgtYF5vp4aqq6t9XzQxkUylefT19/nr5h1sbmxlc2Mbm3e0sqWxjaa2ZMd0I6tKmDl+SPCYMJQpIyupLI1RURynrCT4Y/a0E94TbckUTa1J6ne2s6mxlU0NrWxubGVTWL73t7ewYXsL25sTu32uOGZUlcapLI1TGe5MKkvjlMaLqN3Wwjsf7NwtyIweVMrYIWUMLgt2RoPL4wwuL+7YSQ2rKOl4zrzeF1lJobUn0zz9xib+a+l7/HFdPaXxIj51zHi+cvpUxg8tz2keLe0pnly5kftfXc+r67YRLzJOnzaGC+ZO5COHjTxgTy3ILpsaWnn5na0sfXsrS9/Zyvr6ZgAqSmLMnjSU6oOHc9zk4YwZXMqK2gb+tH4br63fzlubd5AKd/rjhpRx5NhBTBs3OHwMYvKIyj79PsxsubtXd/leAQPBJ4CrgI8DxwO3u/vc3ua5LwLB6vcbufGhFazc0EBJvIgxg0sZMyg4DTN6cCljBpcxZWQlR08YwtjBZf22o+8vTW1JNmxroXZbM7XbWtjY0MrOtiQ725I0hUcxTW1JWhMpDhpaztTRVRw6uorDRldx6KgqhpQXF3oV9ntrNjZy78vv8eDyWszg8o8eyhWnHEJFSddJ9vbmdha/tI7FL61je3OCKSMrWXDcRD5z7ARGDSrdx6WXfWlTQyvL3qtn2bptvLqunjUbG3c7e1BVGueYiUOZPSl4zJowlBFV/f+bKEggMLNfAvOAkcBm4FagGMDd/7cFe8//ILiyqBlY6O697uHzGQjakin+4/m1/OSFtxlaUcw3zzmKj88cu9/t6GX/sWF7C9/97Zv85s/vM2ZwKTeeeSSfOmZ8Rx3Clh2t/Oz373Lvy++xsz3FGdPHcMlHpnD8lOH6XUVUY2uC19ZvZ0tjK7MmDuXQUVV9rkvIRcEygnzIVyBY/l49Nz60krVbmvjMsRP4xtnT+nT+V6Jl2bp6vvXEalbUNjBr4lC+cuphvPBWHQ8sqyGZSnP20QfxpY8dypFjBxe6qBJRCgQ9SKedb/+f1Sx+aR0HDSnnf543k1MO77LJbpEepdPOw69t4HtPvcmWHW0Ux4zPHDuBK045lMkjKwtdPIm4ngLBAXHVUD4tfWcrd/+/dSyonsg3PjmdqtIcNsm29+CDv0BJFZRUQmkVlAwKXlsRtDVCa0P42B4+N0J7E7Q1Bc+Z18lWqBwJg8eHj4NgyITgueQA2Xmk07DjfWjYAI2Zx/vQUAs7P4DismBdMtuotCrYduVDoXw4VAzf/dks3E47oH3nh7dZ59eJ1j0ssAef6/iOwu+ntQHKhsDYmTD2KBh7NIw5CkZOhVjv9SZFRcZn50zgrKPG8tybW6g+eBgH5ViJ3O/cg23X2rD77zFeCpWjoXJUsL2L+rGS3z38vTcGy25vgrYdu16XVMLQSTBkUrDs7k6NpZLQUg8t2yCdhHQKPBX8ztJJ8HTwPyuKBQ/r/FyUNRwPXnsaUu2QbA+eP/RIQLJt12tPQ8WIYDtVjgyeSyr6b1vtqWQbbPxzsP0Gje332Uc+ELz7wU4Arjvj8NyCwMYVcPfHg53U3iqu2BVE4mWw/mVo/uDD08XLgx1T2eDwOXxUjYURh8KIw4LH4IO6/1PlQ7IdNr4O65cGZV//cvDHzVZcCUPGBzud1kZo3Ji1Y2gK/tB9FSsJtmNxObCH619SGWzLipEw/NBgG5cODr6HTSvhlUWQaguXUwpHz4dz78hp1pWlcc6ZdVBu5Ui0Btuy9tUgcCZagkcyfE60BjvBksrwN1O1K5DGS6Fle7DDbKmH5vpdz60Nwed6ZLt2diWV4c42GexwPRXsgM26WXZZsNydW2BnXRDwd9bl/r0Wh0Fh6MTg/9C8NZxPXVB+9sMzFSVVwbYaNwsOPgkmnRAcKPRnMM1o2AC1f4SaV4PnjX8OgtRZ34fjL+/3xUU+ENTUN1MSL2J0LldubHsPfvHZYAdywS+CP0vnI9Z0KtxhD9195102eNfOv6sfTqI1OKpufH/X0XTLtt2PWlu2Qf27wfvJll2fLa4IdmZDJ0LpoF1/3NJB4Y6yLDiiSDQHy0k0B5lIsjXYmcbLg51pcVkwr3hZcETUefpEM2x9BzYsC4YhWO4RH4fxxwZ/7ExWUzak5+CUaA2OHpu37r4Da6kHbFfZS6u6ziZKqiCexzqcVAI++CtsXgVrHofX7oWTrw2yg77YsRne+0PWH3wFpMPLfUsHB9s/+3soLg+OapvrYfv6rN/bjuA7Kq4Is6lhwfOQmcFzedbvrzTrQCLZuvuOe2cdNG0JvtuieKej6niwjMwRfWPtrt96si1YRuVoGDwBxh2z6+i5bMiu76zj+6oIPrt9ffBoqAlfvwftzVA1Oti2B58UzmcUlA/bdUTfcXRfFJTN01kBq1PWkAlimed0MphHrCR8FAfBPfM6XhqOKwnHh9lfc/2ubZTZZo0bYMNyWP1oME3JIJh4HEw6CQ47FQ46ds8Pylobg4OBDX8K5r1hebAcCMpz0Gw4/n/AhLnB9smDyNcRXHnvct7avIPnvzqv5wmb6+FnfxscAX3xaRg9rd/KsMfSadixEbauDR9vB8+NG3ZPxZNdnTKxcKdfHvzI0onwyLM5+HN1JTtYDD4IJp0YHA1NOiH4Aw90jRvhB9Ng3tdg3o17N49UEpb+Oyz55yDTiJcHwXPCcTBxbvC8J9vSPdjB5XDKSvJge02YDYdZ8ZbVgAdBcdrZMO2Twf+k80FfawNsWhUcYGz8c7DTr3uLjgxo2OQgmEycG+z4x87stwMe1RH0YH19M5OG93LuL9ECv7wgOHq5+NHCBgEIjoqGjA8eh5zS/XSpRBAUkq3BUU+8PHju6ojFPTxPGp6SKIpnHZEO/JvFejR4XHAk9sbDexcINq2Cx74cHPUdeTb8zVeDP3hfduJmCgKFNHRi8Dh6fjDcXA9/eTrIHpfdDa/87+C045GfCM7pb1oFm1cGWVBG5ahgpz/jPBg/JzjyrxxRkNWJfCCoqW9mzsE93LKdTsFDl0LNH2H+3TD55H1XuL6KFQfpey7MgiOPeEmQ2svuZnwanvx72LwaxkzP7TPJdvj9v8Hv/zU4zTF/MUz/1L6tz5F9o2I4HHNh8GhrgrXPwJrfwKqHgoOxEYfB+GqY84XgIoSxM6FqzH7zW4h0IGhoDhr06jYjcIff3gBvPgFnfjfYGUg0TT83+C288XBugWDDn+Cxq2DLG3D0guD3UzE8/+WUwiutCvYVMz4d1KWkU4W94igHkW7YJNMGyIRh3XxJf/ghvPqfcNLVcMKV+7Bkst+pGg2T/wZWPUyvzahuWgU/OyOo3L/wAThvkYJAVMVL9/sgAAoEAF1nBIlWeP47wTnd07+1j0sm+6WjzoP6t2HTip6n+913g8sjr/gDHNFT30wi+4dIB4KabUEgmDi8i5t+2ncGl59N/pugclZk2jlBJfqqh7qfZtOq4NzwCVcWrOJPZE9Feg+3vr6Z4ZUlDCrr4uqLzHX6xYXtlUr2IxXD4ZB58MYj3Z8e+t2/BNftn3DFviyZSJ9EOhDU1DczcVg3TQBkmi2IF6iJANk/zTgvuARww/IPv7f5jeDyweOvCK4SEjlAKBB0d8WQMgLpypGfCG6wW/Xwh9/73feCO011YYEcYCIbCFJpp3ZbS/eXjmYyguL9v8Zf9qHyoXDoacHpoXTWndhb1sDqx4KmAHSFkBxgIhsINja0kEx7D4EgqEgmroxAOjnqvKBdqJpXdo373feC9nVO/HLhyiWylyIbCDKXjnZ/aiiTEaiOQDo54qzgAOGN8PTQljeDDGHu5coG5IAU2UBQWx/UAXSfEYR1BMoIpLPSQTD1jOBUUDoFL34/OIV44lWFLpnIXolsIFhf30ysyBg3pJsdvTIC6cmM86BpMyxfHNxXMPcy3TcgB6xIB4LxQ8uJx7rZBMoIpCeH/7cgC/jtDcHBwklXF7pEInst0oGgyzuKM5QRSE9KKuHwM4M+AY67NOiQReQAFdlAULutl34IlBFIb+ZeFjQpfNJXCl0SkT6JZDPUO9uSfNDU3v0VQ7ArI1AgkO4cfBJc8ftCl0KkzyKZEWQam+s1I4iXqcE5ERnwIrmXqwkvHZ3YXT8EEHbvqGxARAa+SAaCHvshyEg0q6JYRCIhkoGgpr6ZQaVxhlb00Pl3QhmBiERDJAPB+rDVUeup4+hkqzICEYmESAaCmt7uIYBdlcUiIgNc5AKBu7O+vpd7CEAZgYhERuQCQd2ONtqS6d4DgTICEYmIvAYCMzvTzN4ys7VmdlMX708ysyVm9pqZrTCzj+ezPLDriqEJyghERIA8BgIziwF3AGcB04ELzWx6p8n+EfiVu88GLgB+nK/yZOR0MxkEGYECgYhEQD4zgrnAWnd/x93bgfuBcztN48Dg8PUQ4P08lgeA9VtbMIPxQ3vZyeuGMhGJiHwGgvFATdZwbTgu223A58ysFngS6LItXzO73MyWmdmyurq6PhVqfX0zYweXUVYc63lC3VAmIhFR6MriC4HF7j4B+DjwX2b2oTK5+yJ3r3b36lGjRvVpgTXbmntuWiJDN5SJSETkMxBsACZmDU8Ix2W7BPgVgLsvBcqAvDbsXhPeTNajdBpSbcoIRCQS8hkIXgWmmtkUMyshqAx+vNM064HTAMxsGkEg6Nu5nx60JlJsamzN7R4CUEYgIpGQt0Dg7kngKuBpYA3B1UFvmNm3zOyccLKvApeZ2Z+BXwJfcHfPV5k2bG/BHSaNyKGiGJQRiEgk5LVjGnd/kqASOHvcLVmvVwMn57MM2WrCewh6rSNQ72QiEiGFrizep2pyaX4asjKCHCqVRUQOcJEKBOvrmymNFzFqUGnPE2YygmJlBCIy8EUuEEzqrflpyKosVh2BiAx8kQoENfUtvV86CsHNZKCMQEQiITKBwN2pyaX5aQhuJgNlBCISCZEJBNubE+xoS+aWESRVRyAi0RGZQJBzq6OgjEBEIiUygSDTD0GvXVSCMgIRiZToBYJcG5wDZQQiEgl5vbN4f7KgeiLVBw+nsjSHVe7ICBQIRGTgi0wgGFFVyoiqXm4ky0io0TkRiY7InBraI8kWiJVCkTaPiAx82tN1JdGqimIRiQwFgq4kmlVRLCKRoUDQlaQyAhGJDgWCriRalBGISGQoEHRFGYGIRIgCQVcSreqURkQiQ4GgK8kW3UMgIpGhQNCVRKvuKhaRyFAg6IoyAhGJEAWCruiGMhGJEAWCruiGMhGJEAWCrujyURGJEAWCztyDQKCMQEQiQoGgs2TYBLUyAhGJCAWCzhKZTml0Q5mIRIMCQWdJdUojItGiQNBZQt1Uiki0KBB0poxARCImr4HAzM40s7fMbK2Z3dTNNOeb2Woze8PM7stneXKS6a9YGYGIRETeOq83sxhwB3AGUAu8amaPu/vqrGmmAl8DTnb3bWY2Ol/lyVkyPDWkjEBEIiKfGcFcYK27v+Pu7cD9wLmdprkMuMPdtwG4+5Y8lic3qiMQkYjJKRCY2cNm9gkz25PAMR6oyRquDcdlOxw43Mz+n5m9bGZndrP8y81smZktq6ur24Mi7IWEMgIRiZZcd+w/Bv4O+KuZfdfMjuin5ceBqcA84ELgp2Y2tPNE7r7I3avdvXrUqFH9tOhuJFVHICLRklMgcPdn3f0i4FhgHfCsmb1kZgvNrLibj20AJmYNTwjHZasFHnf3hLu/C/yFIDAUjk4NiUjE5Hyqx8xGAF8ALgVeA35EEBie6eYjrwJTzWyKmZUAFwCPd5rmUYJsADMbSXCq6J3ci58HHZePKhCISDTkdNWQmT0CHAH8F/BJd98YvvWAmS3r6jPunjSzq4CngRhwl7u/YWbfApa5++Phe39rZquBFPAP7r61b6vURx0ZgeoIRCQacr189HZ3X9LVG+5e3d2H3P1J4MlO427Jeu3A9eFj/6CMQEQiJtdTQ9OzK3HNbJiZfSlPZSqsRAvESqBIN12LSDTkure7zN23ZwbC6/4vy0+RCkx9EYhIxOQaCGJmZpmB8K7hkvwUqcASzaofEJFIybWO4CmCiuE7w+H/EY4beBKtuplMRCIl10BwI8HO/8pw+BngP/NSokJLtugeAhGJlJwCgbungZ+Ej4Et0apAICKRkut9BFOBfwamAx3nTdz9kDyVq3BUWSwiEZNrZfHdBNlAEvgYcA9wb74KVVCJFlUWi0ik5BoIyt39OcDc/T13vw34RP6KVUDKCEQkYnKtLG4Lm6D+a9hsxAagKn/FKiBlBCISMblmBNcAFcBXgDnA54DP56tQBaWMQEQipteMILx5bIG7/z3QBCzMe6kKSTeUiUjE9JoRuHsK+Mg+KMv+QTeUiUjE5FpH8JqZPQ78GtiZGenuD+elVIXiHt5QVlHokoiI7DO5BoIyYCtwatY4BwZWIEi2Bc86NSQiEZLrncUDu14gI5npuF6VxSISHbneWXw3QQawG3f/Yr+XqJASmY7rlRGISHTkemroiazXZcCngff7vzgFpoxARCIo11NDD2UPm9kvgT/kpUSFpIxARCJob/tjnAqM7s+C7BeUEYhIBOVaR7CD3esINhH0UTCwKCMQkQjK9dTQoHwXZL+QUEYgItGT06khM/u0mQ3JGh5qZp/KX7EKJHNqSB3TiEiE5FpHcKu7N2QG3H07cGt+ilRAHaeGFAhEJDpyDQRdTZfrpacHjo7KYtURiEh05BoIlpnZD8zs0PDxA2B5PgtWEMoIRCSCcg0EVwPtwAPA/UAr8OV8FapglBGISATletXQTuCmPJel8JQRiEgE5XrV0DNmNjRreJiZPZ2/YhVIsgWKiqEoVuiSiIjsM7meGhoZXikEgLtvYyDeWZxoVTYgIpGTayBIm9mkzICZTaaL1kgPeIlm1Q+ISOTkGgi+DvzBzP7LzO4Ffgd8rbcPmdmZZvaWma01s27rGMzsM2bmZladY3nyI6mMQESiJ6dA4O5PAdXAW8Avga8CLT19Juz0/g7gLGA6cKGZTe9iukHANcAre1TyfEi0KBCISOTk2ujcpQQ76wnA68AJwFJ277qys7nAWnd/J5zH/cC5wOpO030b+BfgH/ao5PmQVMf1IhI9uZ4augY4DnjP3T8GzAa29/wRxgM1WcO14bgOZnYsMNHd/09PMzKzy81smZktq6ury7HIe0EZgYhEUK6BoNXdWwHMrNTd3wSO6MuCzawI+AHBaaYeufsid6929+pRo0b1ZbE9U0YgIhGUa3tBteF9BI8Cz5jZNuC9Xj6zAZiYNTwhHJcxCDgKeMHMAMYCj5vZOe6+LMdy9a9EK1TmMdCIiOyHcr2z+NPhy9vMbAkwBHiql4+9Ckw1sykEAeAC4O+y5tkAjMwMm9kLwN8XLAhAcEOZMgIRiZg9bkHU3X+X43RJM7sKeBqIAXe5+xtm9i1gmbs/vqfLzjvdUCYiEZTXpqTd/UngyU7jbulm2nn5LEtOdEOZiETQ3nZePzDphjIRiSAFggx3XT4qIpGkQJCRagdcp4ZEJHIUCDIS6rheRKJJgSAjGXZKo4xARCJGgSBDGYGIRJQCQYYyAhGJKAWCDGUEIhJRCgQZyghEJKIUCDISzcFzcUVhyyEiso8pEGQkwoygWBmBiESLAkFGx6kh1RGISLQoEGR0VBYrIxCRaFEgyFBGICIRpUCQoYxARCJKgSBDGYGIRJQCQUaiBYriEMtrXz0iIvsdBYKMZKvuIRCRSFIgyFA3lSISUQoEGYlWVRSLSCQpEGQkW1RRLCKRpECQoYxARCJKgSBDGYGIRJQCQYYyAhGJKAWCDGUEIhJRCgQZyghEJKIUCDJ0Q5mIRJQCQYZuKBORiFIgyEi0quN6EYkkBQIA97CyWBmBiERPXgOBmZ1pZm+Z2Vozu6mL9683s9VmtsLMnjOzg/NZnm6lEuBpVRaLSCTlLRCYWQy4AzgLmA5caGbTO032GlDt7kcDDwLfy1d5epQMO6XR5aMiEkH5zAjmAmvd/R13bwfuB87NnsDdl7h7czj4MjAhj+XpXiLslEYZgYhEUD4DwXigJmu4NhzXnUuA3+axPN1TRiAiEbZfdMdlZp8DqoFTunn/cuBygEmTJvV/AZQRiEiE5TMj2ABMzBqeEI7bjZmdDnwdOMfd27qakbsvcvdqd68eNWpU/5c0kxHohjIRiaB8BoJXgalmNsXMSoALgMezJzCz2cCdBEFgSx7L0rNMRqDLR0UkgvIWCNw9CVwFPA2sAX7l7m+Y2bfM7Jxwsu8DVcCvzex1M3u8m9nlVyKsr9YNZSISQXmtI3D3J4EnO427Jev16flcfs6SyghEJLp0ZzFAIlNHoIxARKJHgQCUEYhIpCkQgDICEYk0BQJQRiAikaZAAFk3lCkjEJHo2S/uLC64ZAsUxSFWXOiSiBxwEokEtbW1tLa2FrooApSVlTFhwgSKi3PfnykQQJARqJ0hkb1SW1vLoEGDmDx5MmZW6OJEmruzdetWamtrmTJlSs6f06khCG4oUztDInultbWVESNGKAjsB8yMESNG7HF2pkAAQWWxMgKRvaYgsP/Ym+9CgQCCy0eVEYhIRCkQQJgRKBCISDQpEECYEejUkIj0LJlMFroIeaGrhkAZgUg/+eZv3mD1+439Os/pBw3m1k/O6HW6T33qU9TU1NDa2so111zD5ZdfzlNPPcXNN99MKpVi5MiRPPfcczQ1NXH11VezbNkyzIxbb72Vz3zmM1RVVdHU1ATAgw8+yBNPPMHixYv5whe+QFlZGa+99honn3wyF1xwAddccw2tra2Ul5dz9913c8QRR5BKpbjxxht56qmnKCoq4rLLLmPGjBncfvvtPProowA888wz/PjHP+aRRx7p123UVwoEEGQE5cMLXQoR6YO77rqL4cOH09LSwnHHHce5557LZZddxosvvsiUKVOor68H4Nvf/jZDhgxh5cqVAGzbtq3XedfW1vLSSy8Ri8VobGzk97//PfF4nGeffZabb76Zhx56iEWLFrFu3Tpef/114vE49fX1DBs2jC996UvU1dUxatQo7r77br74xS/mdTvsDQUCCDICVRaL9FkuRxCOYAwAAA2CSURBVO75cvvtt3ccadfU1LBo0SI++tGPdlxPP3x4cLD37LPPcv/993d8btiwYb3Oe/78+cRiMQAaGhr4/Oc/z1//+lfMjEQi0THfK664gng8vtvyLr74Yu69914WLlzI0qVLueeee/ppjfuPAgHohjKRA9wLL7zAs88+y9KlS6moqGDevHkcc8wxvPnmmznPI/uyy87X4VdWVna8/sY3vsHHPvYxHnnkEdatW8e8efN6nO/ChQv55Cc/SVlZGfPnz+8IFPsTVRZD0MSEMgKRA1ZDQwPDhg2joqKCN998k5dffpnW1lZefPFF3n33XYCOU0NnnHEGd9xxR8dnM6eGxowZw5o1a0in0z2ew29oaGD8+PEALF68uGP8GWecwZ133tlRoZxZ3kEHHcRBBx3Ed77zHRYuXNh/K92PFAggqCNQRiBywDrzzDNJJpNMmzaNm266iRNOOIFRo0axaNEizjvvPGbNmsWCBQsA+Md//Ee2bdvGUUcdxaxZs1iyZAkA3/3udzn77LM56aSTGDduXLfLuuGGG/ja177G7Nmzd7uK6NJLL2XSpEkcffTRzJo1i/vuu6/jvYsuuoiJEycybdq0PG2BvjF3L3QZ9kh1dbUvW7asf2f6zeHwkWvhtFt6n1ZEdrNmzZr9dge3v7jqqquYPXs2l1xyyT5ZXlffiZktd/fqrqbf/05W7WupBHhKGYGI5MWcOXOorKzk3/7t3wpdlG4pEHT0TqY6AhHpf8uXLy90EXqlOgL1TiYiEadA0JERVBS2HCIiBaJAkMkIdGpIRCJKgSCTEaiyWEQiSoGgeWvwrIxARCIq2oGgbQc8dRNUjoJxxxS6NCKyD1RVVRW6CPud6F4+6g6PfRm2vg3//TGoUOujIn3225tg08r+nefYmXDWd/t3nvuBZDK537Q7FN2MYOkdsPoxOP1WmPI3hS6NiOylm266abe2g2677Ta+853vcNppp3Hssccyc+ZMHnvssZzm1dTU1O3n7rnnno7mIy6++GIANm/ezKc//WlmzZrFrFmzeOmll1i3bh1HHXVUx+f+9V//ldtuuw2AefPmce2111JdXc2PfvQjfvOb33D88ccze/ZsTj/9dDZv3txRjoULFzJz5kyOPvpoHnroIe666y6uvfbajvn+9Kc/5brrrtvr7bYbdz+gHnPmzPE+e/cP7rcNc7//Ivd0uu/zE4mw1atXF3T5f/rTn/yjH/1ox/C0adN8/fr13tDQ4O7udXV1fuihh3o6/K9XVlZ2O69EItHl51atWuVTp071uro6d3ffunWru7uff/75/sMf/tDd3ZPJpG/fvt3fffddnzFjRsc8v//97/utt97q7u6nnHKKX3nllR3v1dfXd5Trpz/9qV9//fXu7n7DDTf4Nddcs9t0O3bs8EMOOcTb29vd3f3EE0/0FStWdLkeXX0nwDLvZr+6f+Ql+9KOTfDgQhg+Bc79MWQ1PSsiB57Zs2ezZcsW3n//ferq6hg2bBhjx47luuuu48UXX6SoqIgNGzawefNmxo4d2+O83J2bb775Q597/vnnmT9/PiNHjgR29TXw/PPPd/QvEIvFGDJkSK8d3WQav4Ogw5sFCxawceNG2tvbO/pO6K7PhFNPPZUnnniCadOmkUgkmDlz5h5ura7l9dSQmZ1pZm+Z2Vozu6mL90vN7IHw/VfMbHI+y0MqAb/6fFBJvOBeKBuc18WJyL4xf/58HnzwQR544AEWLFjAL37xC+rq6li+fDmvv/46Y8aM+VAfA13Z289li8fjpNPpjuGe+ja4+uqrueqqq1i5ciV33nlnr8u69NJLWbx4MXfffXe/Nmmdt0BgZjHgDuAsYDpwoZlN7zTZJcA2dz8M+CHwL/kqDwDP3AI1L8M5/w6j1VqiyECxYMEC7r//fh588EHmz59PQ0MDo0ePpri4mCVLlvDee+/lNJ/uPnfqqafy61//mq1bg8vNM30NnHbaafzkJz8BIJVK0dDQwJgxY9iyZQtbt26lra2NJ554osflZfo2+PnPf94xvrs+E44//nhqamq47777uPDCC3PdPL3KZ0YwF1jr7u+4eztwP3Bup2nOBTJr/yBwmlmeztWsehhe/jEcfwXM/GxeFiEihTFjxgx27NjB+PHjGTduHBdddBHLli1j5syZ3HPPPRx55JE5zae7z82YMYOvf/3rnHLKKcyaNYvrr78egB/96EcsWbKEmTNnMmfOHFavXk1xcTG33HILc+fO5Ywzzuhx2bfddhvz589nzpw5HaedoPs+EwDOP/98Tj755Jy62MxV3vojMLPPAme6+6Xh8MXA8e5+VdY0q8JpasPht8NpPug0r8uBywEmTZo0J9fovpt3XoBXFsH8xRAv2at1EpEPU38E+9bZZ5/Nddddx2mnndbtNHvaH8EBcfmouy9y92p3rx41atTezeSQeXDhfQoCInJA2r59O4cffjjl5eU9BoG9kc+rhjYAE7OGJ4Tjupqm1sziwBBgax7LJCLCypUrO+4FyCgtLeWVV14pUIl6N3ToUP7yl7/kZd75DASvAlPNbArBDv8C4O86TfM48HlgKfBZ4HnP17kqEckbdydf1Xv5MHPmTF5//fVCFyMv9mYXmrdTQ+6eBK4CngbWAL9y9zfM7Ftmdk442c+AEWa2Frge+NAlpiKyfysrK2Pr1q17tQOS/uXubN26lbKyPWtEU53Xi0ifJBIJamtr9/h6e8mPsrIyJkyYQHFx8W7j1Xm9iORNcXFxxx2xcmA6IK4aEhGR/FEgEBGJOAUCEZGIO+Aqi82sDtiLW4sBGAl80OtUA09U1xuiu+5a72jJZb0Pdvcu78g94AJBX5jZsu5qzQeyqK43RHfdtd7R0tf11qkhEZGIUyAQEYm4qAWCRYUuQIFEdb0huuuu9Y6WPq13pOoIRETkw6KWEYiISCcKBCIiEReZQGBmZ5rZW2a21swGbCunZnaXmW0Je3/LjBtuZs+Y2V/D5/7r424/YWYTzWyJma02szfM7Jpw/IBedzMrM7M/mtmfw/X+Zjh+ipm9Ev7eHzCzAdkjk5nFzOw1M3siHB7w621m68xspZm9bmbLwnF9+p1HIhCYWQy4AzgLmA5caGbTC1uqvFkMnNlp3E3Ac+4+FXiOgdncdxL4qrtPB04Avhx+xwN93duAU919FnAMcKaZnQD8C/BDdz8M2AZcUsAy5tM1BM3cZ0RlvT/m7sdk3TvQp995JAIBMBdY6+7vuHs7cD9wboHLlBfu/iJQ32n0ucDPw9c/Bz61Twu1D7j7Rnf/U/h6B8HOYTwDfN090BQOFocPB04FHgzHD7j1BjCzCcAngP8Mh40IrHc3+vQ7j0ogGA/UZA3XhuOiYoy7bwxfbwLGFLIw+WZmk4HZwCtEYN3D0yOvA1uAZ4C3ge1h51AwcH/v/wu4AUiHwyOIxno78H/NbLmZXR6O69PvXP0RRIy7u5kN2GuGzawKeAi41t0bs7tPHKjr7u4p4BgzGwo8AhxZ4CLlnZmdDWxx9+VmNq/Q5dnHPuLuG8xsNPCMmb2Z/ebe/M6jkhFsACZmDU8Ix0XFZjMbBxA+bylwefLCzIoJgsAv3P3hcHQk1h3A3bcDS4ATgaFmljnQG4i/95OBc8xsHcGp3lOBHzHw1xt33xA+byEI/HPp4+88KoHgVWBqeEVBCXAB8HiBy7QvPQ58Pnz9eeCxApYlL8Lzwz8D1rj7D7LeGtDrbmajwkwAMysHziCoH1kCfDacbMCtt7t/zd0nuPtkgv/z8+5+EQN8vc2s0swGZV4Dfwusoo+/88jcWWxmHyc4pxgD7nL3fypwkfLCzH4JzCNolnYzcCvwKPArYBJBE97nu3vnCuUDmpl9BPg9sJJd54xvJqgnGLDrbmZHE1QOxggO7H7l7t8ys0MIjpSHA68Bn3P3tsKVNH/CU0N/7+5nD/T1DtfvkXAwDtzn7v9kZiPow+88MoFARES6FpVTQyIi0g0FAhGRiFMgEBGJOAUCEZGIUyAQEYk4BQKRkJmlwhYdM49+a6DOzCZntwgrsj9RExMiu7S4+zGFLoTIvqaMQKQXYfvv3wvbgP+jmR0Wjp9sZs+b2Qoze87MJoXjx5jZI2EfAX82s5PCWcXM7KdhvwH/N7wTGDP7StiPwgozu79AqykRpkAgskt5p1NDC7Lea3D3mcB/ENyhDvDvwM/d/WjgF8Dt4fjbgd+FfQQcC7wRjp8K3OHuM4DtwGfC8TcBs8P5XJGvlRPpju4sFgmZWZO7V3Uxfh1B5y/vhA3bbXL3EWb2ATDO3RPh+I3uPtLM6oAJ2U0bhE1jPxN2HIKZ3QgUu/t3zOwpoImgKZBHs/oXENknlBGI5Ma7eb0nstu8SbGrju4TBD3oHQu8mtV6psg+oUAgkpsFWc9Lw9cvEbR8CXARQaN3EHQVeCV0dBozpLuZmlkRMNHdlwA3AkOAD2UlIvmkIw+RXcrDnr4ynnL3zCWkw8xsBcFR/YXhuKuBu83sH4A6YGE4/hpgkZldQnDkfyWwka7FgHvDYGHA7WG/AiL7jOoIRHoR1hFUu/sHhS6LSD7o1JCISMQpIxARiThlBCIiEadAICIScQoEIiIRp0AgIhJxCgQiIhH3/wHHV0FIlQoLuAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxdVb338c/vDDlJM3RI03SeGAUqRQuCUlSc0CugIvYi6pVHLy9REYfLFaf7qA9e79V7VRyuPDgPKPCAOAsOVAFFpC1lhspQoHNS2iZpm+mc3/PH2idNS5qkbXZOsvf3/Xqd1z7zXjvZ53vWWXvttczdERGR5MlUugAiIhIPBbyISEIp4EVEEkoBLyKSUAp4EZGEylW6AP1NnTrV58+fX+liiIiMGytXrmx196aBHhtTAT9//nxWrFhR6WKIiIwbZvbk/h5TE42ISEIp4EVEEkoBLyKSUGOqDV5E0qenp4d169bR2dlZ6aKMadXV1cyePZt8Pj/s1yjgRaSi1q1bR319PfPnz8fMKl2cMcnd2bp1K+vWrWPBggXDfp2aaESkojo7O2lsbFS4D8LMaGxsPOBfOQp4Eak4hfvQDuZvNP4DvlSCW/8LHv19pUsiIjKmjP+Az2TgL1+BR35T6ZKIyDhVV1dX6SLEYvwHPMCkubD9qUqXQkRkTFHAi4hE3J1LL72U4447jkWLFnHttdcCsHHjRk477TQWL17Mcccdx2233UaxWOTtb39733O/+MUvVrj0z5aMbpKT5sFjt4A76GCNyLj1qV88wIMb2kb0PY+Z2cD/PvPYYT33Jz/5CatXr+aee+6htbWVE088kdNOO40f/ehHvOpVr+JjH/sYxWKRXbt2sXr1atavX8/9998PwPbt20e03CMhGTX4yfOgZxfsbK10SURkHLv99ts577zzyGazNDc38+IXv5i77rqLE088ke985zt88pOf5L777qO+vp6FCxfy+OOPc/HFF3PTTTfR0NBQ6eI/S0Jq8HPDcvtTUDfgqJkiMg4Mt6Y92k477TRuvfVWfvWrX/H2t7+dD37wg7ztbW/jnnvu4eabb+bKK6/kuuuu49vf/nali7qXZNTg+wJ+bUWLISLj29KlS7n22mspFou0tLRw6623ctJJJ/Hkk0/S3NzMP//zP/POd76TVatW0draSqlU4pxzzuHyyy9n1apVlS7+sySvBi8icpBe//rXc8cdd3D88cdjZnzuc59j+vTpfO973+Pzn/88+Xyeuro6vv/977N+/XouuOACSqUSAJ/97GcrXPpnM3evdBn6LFmyxA96wo//XADHvg5eO/aOZIvI/j300EM85znPqXQxxoWB/lZmttLdlwz0/GQ00YC6SoqI7CO2gDezo8xsdb9Lm5m9P671MWkubNvvzFUiIqkTWxu8uz8CLAYwsyywHrgxrvUxeR78/bfqCy8iEhmtJpqXAY+5e3xV7EnzoLcTOrbEtgoRkfFktAL+H4EfD/SAmV1oZivMbEVLS8vBr0E9aURE9hJ7wJtZFXAW8P8Getzdr3L3Je6+pKnpEE5S6gt4tcOLiMDo1OBfDaxy982xrkUBLyKyl9EI+PPYT/PMiKqqhQlT1UQjIrEabOz4tWvXctxxx41iaQYXa8CbWS3wCuAnca6nj7pKioj0iXWoAnffCTTGuY69TJoLm+4btdWJyAj7zWUj/xmevghe/R/7ffiyyy5jzpw5vOc97wHgk5/8JLlcjuXLl7Nt2zZ6enq4/PLLOfvssw9otZ2dnVx00UWsWLGCXC7HF77wBV760pfywAMPcMEFF9Dd3U2pVOKGG25g5syZvOlNb2LdunUUi0U+8YlPsGzZskPabEjKWDRlk+fBI78O87RmknOSrojEZ9myZbz//e/vC/jrrruOm2++mfe97300NDTQ2trKySefzFlnnXVAE19/7Wtfw8y47777ePjhh3nlK1/JmjVruPLKK7nkkks4//zz6e7uplgs8utf/5qZM2fyq1/9CoAdO3aMyLYlK+AnzYViN3RshoYZlS6NiByoQWracTnhhBPYsmULGzZsoKWlhcmTJzN9+nQ+8IEPcOutt5LJZFi/fj2bN29m+vTpw37f22+/nYsvvhiAo48+mnnz5rFmzRpOOeUUPvOZz7Bu3Tre8IY3cMQRR7Bo0SI+9KEP8eEPf5jXvva1LF26dES2LVnV3EnzwlI9aUTkAJx77rlcf/31XHvttSxbtoyrr76alpYWVq5cyerVq2lubqazs3NE1vXmN7+Zn//859TU1PCa17yGW265hSOPPJJVq1axaNEiPv7xj/PpT396RNaVsIDXyU4icuCWLVvGNddcw/XXX8+5557Ljh07mDZtGvl8nuXLl/PkkwdeaVy6dClXX301AGvWrOGpp57iqKOO4vHHH2fhwoW8733v4+yzz+bee+9lw4YNTJgwgbe85S1ceumlIza2fPKaaEA1eBE5IMceeyzt7e3MmjWLGTNmcP7553PmmWeyaNEilixZwtFHH33A7/nud7+biy66iEWLFpHL5fjud79LoVDguuuu4wc/+AH5fJ7p06fz0Y9+lLvuuotLL72UTCZDPp/n61//+ohsV3LGgy/7/BFw1Blw1ldGplAiEiuNBz986R0Pvkx94UVEgKQ10UAI+A13V7oUIpJg9913H29961v3uq9QKHDnnXdWqEQDS17AT54HD/0CSkXIZCtdGhEZBnc/oD7mlbZo0SJWr149qus8mOb0ZDbRlHqgfVOlSyIiw1BdXc3WrVsPKsDSwt3ZunUr1dXVB/S65NXg+/ekmTirsmURkSHNnj2bdevWcUjzQaRAdXU1s2fPPqDXJDDg54fl9qdg3gsrWhQRGVo+n2fBggWVLkYiJa+JZmL0DaeTnUQk5ZIX8PlqqJuurpIiknrJC3gI7fA6m1VEUi6ZAT95nppoRCT1khnwk+ZC23oo9la6JCIiFRP3lH2TzOx6M3vYzB4ys1PiXF+fSXOh1AvtG0ZldSIiY1HcNfgrgJvc/WjgeOChmNcXaNhgEZH4+sGb2UTgNODtAO7eDXTHtb699E38oYAXkfSKswa/AGgBvmNmd5vZN82sdt8nmdmFZrbCzFaM2JlsE2cDpoAXkVSLM+BzwPOAr7v7CcBO4LJ9n+TuV7n7Endf0tTUNEJrLkD9DPWFF5FUizPg1wHr3L08fub1hMAfHZPmqgYvIqkWW8C7+ybgaTM7KrrrZcCDca3vWdQXXkRSLu5eNBcDV5vZvcBi4N9jXt8ek+ZC2zoo9ozaKkVExpJYR5N099XAgHMFxm7SXPBSOOFp8vyKFEFEpJKSeSYrqKukiKReggNeJzuJSLolN+AbZoFl1FVSRFIruQGfq4L6marBi0hqJTfgASbNgR3rKl0KEZGKSHbAV0+Erh2VLoWISEUkO+Cr6qCro9KlEBGpiGQHfKEOuhXwIpJOyQ541eBFJMWSHfCFBujdran7RCSVEh7wdWGpZhoRSaFkB3yVAl5E0ivZAV+uwasdXkRSKNkBX1Ufll3tlS2HiEgFJDvg+9rgFfAikj7JDvgqNdGISHolO+DVi0ZEUizZAd/XBq+AF5H0iXXKPjNbC7QDRaDX3Ud3+r5CFPBqgxeRFIo14CMvdffWUVjPs+UKkMmpBi8iqZTsJhqzcKBVbfAikkJxB7wDvzWzlWZ24UBPMLMLzWyFma1oaWkZ+RIU6lWDF5FUijvgT3X35wGvBt5jZqft+wR3v8rdl7j7kqamppEvQVUddLWN/PuKiIxxsQa8u6+PlluAG4GT4lzfgDQmvIikVGwBb2a1ZlZfvg68Erg/rvXtl8aEF5GUirMXTTNwo5mV1/Mjd78pxvUNrFAH7RtHfbUiIpUWW8C7++PA8XG9/7BV6SCriKRTsrtJQuhFoxOdRCSFUhDwURu8e6VLIiIyqpIf8FV14EXo7ax0SURERlXyA76gAcdEJJ2SH/B9Y8LrZCcRSZfkB7zGhBeRlEp+wGtWJxFJqeQHfN+Y8Ap4EUmX9AR8l/rCi0i6JD/gq9QGLyLplPyAL6gNXkTSKfkBrxq8iKRU8gM+k4X8BLXBi0jqJD/gIRoTXgEvIumSjoDXrE4ikkLpCHjN6iQiKZSOgC/UqwYvIqkTe8CbWdbM7jazX8a9rv0q1KsNXkRSZzRq8JcAD43CevavSm3wIpI+sQa8mc0G/gH4ZpzrGVJBbfAikj5x1+C/BPwrUNrfE8zsQjNbYWYrWlpa4imFavAikkLDCngzu8TMGiz4lpmtMrNXDvGa1wJb3H3lYM9z96vcfYm7L2lqajqAoh+AQj307IJibzzvLyIyBg23Bv+/3L0NeCUwGXgr8B9DvOZFwFlmtha4BjjdzH54sAU9JBquQERSaLgBb9HyNcAP3P2BfvcNyN0/4u6z3X0+8I/ALe7+loMu6aHQrE4ikkLDDfiVZvZbQsDfbGb1DNKuPuZoVicRSaHcMJ/3DmAx8Li77zKzKcAFw12Ju/8R+OMBl26kaFYnEUmh4dbgTwEecfftZvYW4OPAjviKNcI0q5OIpNBwA/7rwC4zOx74EPAY8P3YSjXSdJBVRFJouAHf6+4OnA181d2/BtTHV6wRplmdRCSFhtsG325mHyF0j1xqZhkgH1+xRliV2uBFJH2GW4NfBnQR+sNvAmYDn4+tVCOtrwbfVtlyiIiMomEFfBTqVwMTozNUO919/LTB56rBsmqiEZFUGe5QBW8C/gacC7wJuNPM3hhnwUaUmWZ1EpHUGW4b/MeAE919C4CZNQG/B66Pq2AjrqpeNXgRSZXhtsFnyuEe2XoArx0bCvXQrX7wIpIew63B32RmNwM/jm4vA34dT5FiojHhRSRlhhXw7n6pmZ1DGCES4Cp3vzG+YsVAY8KLSMoMtwaPu98A3BBjWeJVqIP2TZUuhYjIqBk04M2sHfCBHgLc3RtiKVUcqupVgxeRVBk04N19/AxHMJRCnU50EpFUGV89YQ5FVXSQ1Qf6QSIikjzpCfhCHXgRejsrXRIRkVGRnoAvDzimrpIikhLpCfi+WZ10spOIpENsAW9m1Wb2NzO7x8weMLNPxbWuYdGY8CKSMsPuB38QuoDT3b3DzPLA7Wb2G3f/a4zr3D/N6iQiKRNbwEczQJXTNB9dKteFpaA2eBFJl1jb4M0sa2argS3A79z9zgGec6GZrTCzFS0tLfEVpq8GrzZ4EUmHWAPe3YvuvpgwA9RJZnbcAM+5yt2XuPuSpqam+ArT1wavgBeRdBiVXjTuvh1YDpwxGusbUJUOsopIusTZi6bJzCZF12uAVwAPx7W+Iekgq4ikTJy9aGYA3zOzLOGL5Dp3/2WM6xtcNgf5CWqiEZHUiLMXzb3ACXG9/0HRmPAikiLpOZMVNKuTiKRKugJeNXgRSZF0BXyhXjV4EUmNdAV8VZ1OdBKR1EhXwBfq1ItGRFIjXQFfpYOsIpIe6Qr4gibeFpH0GPcBXyo5P717Pauf3j70kwv10LMLSsX4CyYiUmHjPuDN4KM33sfPV28Y+skarkBEUiQBAW9Mb6hmc9swJtPWrE4ikiLjPuABmocb8KrBi0iKJCTgC2waVg1eszqJSHokI+AnVrOlrYswS+AgNKuTiKRIIgJ+ekM13cUS23b1DP5EzeokIimSiIBvbqgGYNOOIZppNKuTiKRIogJ+yAOt5TZ4HWQVkRRISMAXgAMIeDXRiEgKxDkn6xwzW25mD5rZA2Z2SVzrmlYfNdEMFfC5arCsavAikgpxzsnaC3zI3VeZWT2w0sx+5+4PjvSKqnIZptZVDV2DN9OsTiKSGrHV4N19o7uviq63Aw8Bs+Ja37T6aja3dQ39xCoNOCYi6TAqbfBmNp8wAfedAzx2oZmtMLMVLS0tB72O6ROrh+5FAxoTXkRSI/aAN7M64Abg/e7etu/j7n6Vuy9x9yVNTU0HvZ7mhmq2tA9zuALV4EUkBWINeDPLE8L9anf/SZzram4o0NrRTXdvafAnqgYvIikRZy8aA74FPOTuX4hrPWXTo77wQ9biNauTiKREnDX4FwFvBU43s9XR5TVxrWzPyU5DHGgtNKiJRkRSIbZuku5+O2Bxvf++hn82q5poRCQdEnEmK4ReNDDM8Wi6O2CokSdFRMa5xAT85Al5qrIZNg/VBl+og1Iv9A6jz7yIyDiWmIA3M6Y1FNg8ZA1eA46JSDokJuAh9KQZcjwajQkvIimRqIBvbggzOw1K87KKSEokLuA3tXUOPnWfavAikhIJC/gCu7qLdHT17v9JVZp4W0TSIVEBX+4qOWhf+L5ZnVSDF5FkS1TA75mbdZB2+ILmZRWRdEhkwA9ag9dBVhFJiUQFfHnAsUG7SlapBi8i6ZCogK+pytJQnRu8Bp/NQa5GbfAikniJCngIzTTDG3BMNXgRSbbEBfz0idVsGs7JTmqDF5GES1zANzdUDz0ejYYMFpEUSGDAF2jp6KJYGuRs1qp6NdGISOIlLuCnN1RTLDlbOwbrC1+vg6wiknhxzsn6bTPbYmb3x7WOgUwbztR9OsgqIikQZw3+u8AZMb7/gIbdF14HWUUk4WILeHe/FXgmrvffn76p+4Yaj6ZzB5SKo1QqEZHRl7g2+MbaKjIGWwYL+FnPg95OePrO0SuYiMgoq3jAm9mFZrbCzFa0tLQc8vvlshma6guDT759xCshW4CHfnHI6xMRGasqHvDufpW7L3H3JU1NTSPynkNO3Veoh8NODwE/2OQgIiLjWMUDPg7ThjN133POhB1Pw8bVo1MoEZFRFmc3yR8DdwBHmdk6M3tHXOva17Am3z7q1WBZNdPIwFb9AO7/iX7hybgWZy+a89x9hrvn3X22u38rrnXtq7mhwI7dPXT2DNJLZsIUmH+qAl6ereUR+PnFcP0FcPW5sP3pSpdI5KAksolmWBN/QGimaV0TPtAiZbf+F+Rr4GX/Bk/+Gf7nZLjzKiiVKl0ykQOSyIDv6ws/1KBjzzkTMHjw5/EXSsaHrY/B/dfDie+ApR+Cd98Bs0+E31wK3zlDlQEZVxIZ8H01+PYhDrTWT4c5J8FDCniJ3PbfkK2CUy4OtyfPh7feCK/7egj3K0+FP1+htnkZF5Id8EPV4CHU4jfdC9vWxlsoGfu2rYV7roHnXwD1zXvuN4PFb4b33hXOofjdv8FvP66QlzEvkQHfUJ2jJp8duicNwNGvDcuHfhlvoWTsu+0LkMnBiy4Z+PG6afCmH8BJF8IdX4VffkDt8jKm5SpdgDiYGc0NhaEPsgJMWQDTF4XeNC98b/yFk7Fp+9Ow+kfw/H+Chhn7f14mA6/+HFTVwu1fhJ7dcPbXwly/kh5P/gVuuRyyeahrhtqmUAGoaw7LWUuguqHSpUxmwMMw52Yte87ZsPwz0L4ptMtL+vz5S2H5ovcP/VwzePknw6ikt/wf6NkJ53wLcoU4SyhjgTv89X/gt5+AhpkhL566AzpaoHf3nuflJ8AxZ8Pi82Hei0LFoAISHfB3P71teE9+zpmw/HJ4+Jdw4jvjLZiMPW0bYNX3Qzv7pDnDf91p/xJq8jddBte8OTTfVE2Ir5xSWV0d8PP3wgM3hqbd1/0PVE8Mj7mHaUB3tsD2p+DBn8J9N8A9Pw4H6hefD8efd2D71whIZBs8hK6Sm9u68OEcCGs6ChqP0ElPafXnK8LQ0Us/eOCvPfkiOOsr8Ogf4Aevh02jOr+NjJaWNfCN0+HBn4Vfb8t+uCfcIfyqq26AxsPgsJfCmVfAv6yB118FE+eEFoIvLYJrzh/VrraJrsF395bYvquHybVVgz/ZLNTi/3wF7HomnOU6Vu3eBo//CR79fVjmq2HBi2Hhi8OZuTWTK13C8aV9M6z8bqhdTZ5/cO/xvLeFmvwvPgBXvgiOeR285CMw7eiRLKlUyoM/g5++OzTBvfVGWPiS4b2uagIcvyxctq0Nw1/c+X/hkZPhhLeEfaRhZowFT3TAh/bQTW2dQwc8hIC//Quw5qbwU73S3KF7J+zcAm0bYe3t8NgfYN1d4CUoTISFp0FPJ6y+Gu76BlgGZhwfdsAZi8OomYX60FZcqAvLXHU4MNizK1y6d4ZlsSe8dix/ucXhL1+GYvfB1d77O+6cMELpHV+Dv349hMKic+HFH4aph49MWceL9s3w+PIw30Lj4aECMu2YirVDD5s7tK2HzQ/A5vvDctP90PoIzHo+vOn7MHH2wb335Pnwsk+EX3y3/Tf87Rtw73XwgnfBqe+PrWJmw2rCGCVLlizxFStWjMh7rVj7DG+88g6+e8GJvOSoaUO/wD38hJq+CM778bMf6+0K3+Bmh164rg5o3xh2prYN0XJjuK9jSwj1fQ/aYDDzBDj85XD4y8JR+nLPjd5uWL8i1Ogf/2O4Xuo9iIIZzHhu9IvgJTD3lOG3Kfd27dmWXVvD7d7OfsvogPeUhTD1yPDBz9ccRBkPUakYPrxrbw+XR38Px74e3nDVyK1j51b4yxXhQ9zbGd6/tin6Yt0d/q89neG6F6P+9L5nCeEgXW1TdGncc73QEL7ILRP2xfJ1bO/38NKefvrZKshVhTkQ+pbRvlx+Tv91Q3g/iy5917OhG2k2v2f9ELbjqTvgsVvgseXh7wuQrw0HoAEmNML8peGX5oIXh9v9Kxrl68X97LdGtM4sZLJ7rveVw8Jz+sqeCduYrwl/y3wN5GrCZ6ZzBzzzeDhr+Zkn4JnHwu2WR6Bz+551TpoLzcfBnBeEYB7Jg+jb1sLyfw8hXz0xnDX9gneF/88BMrOV7r5kwMeSGvBPP7OLpZ9bzn+es4hlJ84d3otu+gjc9c0Qbru3h+aQzu3heqknfDDqm6Fu+t5Ly0ZBFn1wy8vundDVFi6d0bKrPdQY91UzJfxcK3e3Kn+g66ZB7bQQ7rWNw9uOrvawA3V1hLlnu9qjZUcInPyEENz5CaFpIT8BcHjqTnjiT/D036LtrYLZJ4WeAv3DpBwondtDoO9YH76UDoiFA05TjwzHP+qawt+gZvLeFwhl7tkVheKu6MvCQtmravv9QqkNAdTZFj7EnTuiv/2O8OX55B1hbJnyh3jKwhA6p388/J1HWscWuP1LcPcPAY/CJgqafE34NZXJ7h2i5ZTq3hkO2O3aGrZhLMrkw9+71BMqFNkqmHty+CVz2OnQvCjsH2tvgyduDRWQ9g0VLnPu2ZWf+plRxePwEOjNx0HzMXu3scdl473wh0+FA7MX3XFQ3W1TGfBdvUWO+vhNfODlR3LJy48Y3os2PwDXvyN8U9dMgupJYVkzOTR17N4OHZtDd8r2TdCxKYRHWbYQ2sRzNWGZrw0HXgoN4fXl6zWToGFW1M1qRlhWoja7P907Qxg+8cdQy+1sCzXC/jXDUjFs08RZYVsmztlzvbYpCrDCntpirjrUVrc+FgZ4a/17tFwT7ivX9OI0ZWE4TjF/aei6NnFW/OscCT2dsKsVdraGsPfy/6H8P4luD1TrhtD81tsVKha9XVDsCr/6+mrs+7xm318U5f+5F0M4lorhPUu9IdwzeZj3wnCpqt3/driH//Xa28IXdb4mfEbKteyqCeG9BnxttL1eDMtSsd99/X599C9zb9eeXwa9nXuuV08K+0LjYaHpZLAyj5bd2w66mWawgE9sG3whl2VKbRWb24fZFx6g+Vh4z18PbEU9nWFnylWP/TbG4aqqhSNeHi4jbfpx4bKvnt3Rr6Znws5evkD48Oeq+9WAq4HoGEXfpSMsiz3hi7R6YjhOUb4+oRFqp4789oyGfHVo+z3Y9t+xwizUktN2TGI4YmqDT2zAQ+hJs/qp7Ty8qY2jp8d0Vlm+Op73TZtyeA92FmkFuTu/e3AzX13+KBt3dHLq4VN5yVFNnHr4VBrrdIKTjE2JDvgzjp3Ol2/5O2d86TaOnl7P60+YxVmLZzJj4hhqDpExzd357YOb+fIf/s4DG9qY1ziBkxc28qc1Ldx493rM4LmzJvLiI5t46dHTWDxnEjYSB+JFRkBi2+DLtnZ08ct7N3Lj3etZ/fR2zODkBY284phmZk+uYcbEGponFphaWyCTGb8fzO7eEuu27eKpZ8Klu7fEgqm1LJhay5wpE8hnE9J8NErKwX7F7//OgxvbmN84gfeefgSvWzyTXDZDseTcv34Hf1rTwp/WtHD3U9soOSycWss5z5/NOc+b3TcvgSRbqeRs29XNlvYuNrd10trRTank5LJGNmPksxly0XLOlBoOa6ob0UpAxQ6ymtkZwBVAFvimu//HYM+PI+D7e6J1Jz9bvZ6f3r2etVt37fVYLmNMqy8wY1INC6bWclhTHYc11bKwqY55jaMbkMVoh9na0c3Wji627uymo6uXnV297OwqsrO7fL2XzW1dPPXMLjbs2L3f0WuzGWPulAksjMJ+Yk2e+uocDTV5Gqpz1FfnaajO01CTix7Lkx3HX3YHa2dXL3c+sZVb17TypzUtPNG6kwVTa3nvSw/n7CjY92fHrh5ufnAT169Yx9/WPkPGYOkRTZy7ZDavOKaZQi47ilsicejsKfLwpnbuX7+DBza08fCmNjbv6GRLexe9peHn6LT6Ai88rJEXHjaVUw5rZM6UQxveoiIBb2ZZYA3wCmAdcBdwnrs/uL/XxB3wZe7OlvYuNu3oZFNbJ5vbOvuur9+2mydad7Kl32QhuYwxZ8oECrkMJXdKDiX30JnEnYyFb+pcxqJv7fCNXb5vzzLcbwY9xRJdvSW6e0t0F0v0FEt09pR4Zmc323Z1DzrUeCGXoa6QY0IhS1NdgXmNIbjnTZnA3MawzGczPN66kydad/JEawePt4Tr67btpqNr6D7y9YXoC6Amz9S6KpobqmluKNDcUM20+nC9Op+lJyp7V2+JnqLT01vq29nLlRQjjPCZsXDwu6YqQyGXpTqfpaYqS3UuQzZjZDJGJnpeWIa/XcYYssbj7hRL4X8zkJI73cUSXT0lunqLfX/79s5e7lr7DLf9vYWVT26jp+gUchlesLCR1y2eyVnHDx7sA1nbupPrV67jhlXr2Lijk/pCjjlTJjCtocC0+gLT6qv7rk+oylGVy4RLNkMhup6N/hYW/S3MwAj7UT6XIZ818pnMgL86SyWnpxT+H8Wik83aoM8faaWSU4z+Hz3FEr1Fp7fk9Il0U20AAAjvSURBVJb2XM9ljEI+Q3U+S3UuSz5ro9q01Vss0dlbYnd3kc6eIrt7inR09dK2u4e2zvKyh7bdvWxp6+SBDW082tJBMdrBGqpzHDOzgZmTaqLPRKFv2VRfIJuxZ213d7HEmk3t/PmxrdzxWCutHaG79JwpNZx6+FQuf92ig6pYVSrgTwE+6e6vim5/BMDdP7u/14xWwA9HW2cPT7Ts5LGWKBy37qS3WOoLnv4fPHf6/onFUvinlnfuku+5XX685E4+m+n7YBfKH+58hskTqmisK9BYW0VjXRVTaqtorC3QUJNjQlWO2qrsAQfOvoolp6OzN+zA0U7c1tlDe2cvO3b30La7Jyw7w/WWjm62tIWaSvEAaiojKdv/izKk3V5/60Mt1zEzGlh65FROO6KJ58+bTHX+0GvcxZLzl8dauen+TWzc0cmW9k62tHXR2tG13y+iAxWaAIxcJtP3ZTvYe5ebCvJZI5fNDFAJCQFTrrwU3SmVwhdoyaHo3ne9FIV4ef8v/z8OJlIyBtX5LPlsZs9ni/DFHv27+77kyreJHi9/psrlKZepXIz+GecQ/Z2GV8h81misLfCcGfUcN2six85s4NiZE5k9ueaQvpDcnb9v6eAvj7byl8e2sn1XD9e965SDeq9KBfwbgTPc/Z3R7bcCL3D39+7zvAuBCwHmzp37/CeffDKW8sihK5acZ3Z2s7kthFV3bykKi8yeL6xsZq/eov13r2LJ6eotsbsn1Jr2XEp9H9K9gyUESQjxEsUSFEulvhDJD/BrabAaaiGXoZDPUoi+TMu3j5s5kab60esJUyw5W3d20dLexe7uIt29JbqK0a+56FIsOU7Yfu/7xRjVCKPaYE/fJVQmqrIZclHYV+X2/F1K7vQUne7eEr1Rzb68jvLftn8lhL5fUHt+SWWiLvLZvl9Ze35ZZSx8yWT2+bLIZMKvhlz0ZZLr9yu3t+h09pbo6rcPdPYU6SmWcPptMyEMvdzdnz1fIOXnZTOhXGbhyz+b2VMBK+u/V+RzGWryWWryWaqrsn3XJ1RlaajJM7EmFzVZ5inkMmP+oPmY7gfv7lcBV0GowVe4ODKIbMZoin6Cwiic5ZdQ2YyFZpp6HYSVeMV55HA90H/w49nRfSIiMgriDPi7gCPMbIGZVQH/CPw8xvWJiEg/sTXRuHuvmb0XuJnQTfLb7v5AXOsTEZG9xdoG7+6/Bn4d5zpERGRgOr1RRCShFPAiIgmlgBcRSSgFvIhIQo2p0STNrAU42FNZpwKtI1ic8ULbnS7a7nQZznbPc/emgR4YUwF/KMxsxf5O100ybXe6aLvT5VC3W000IiIJpYAXEUmoJAX8VZUuQIVou9NF250uh7TdiWmDFxGRvSWpBi8iIv0o4EVEEmrcB7yZnWFmj5jZo2Z2WaXLEycz+7aZbTGz+/vdN8XMfmdmf4+WkytZxpFmZnPMbLmZPWhmD5jZJdH9id5uADOrNrO/mdk90bZ/Krp/gZndGe3z10bDcSeKmWXN7G4z+2V0O/HbDGBma83sPjNbbWYrovsOel8f1wEfTez9NeDVwDHAeWZ2TGVLFavvAmfsc99lwB/c/QjgD9HtJOkFPuTuxwAnA++J/sdJ326ALuB0dz8eWAycYWYnA/8JfNHdDwe2Ae+oYBnjcgnwUL/badjmspe6++J+/d8Pel8f1wEPnAQ86u6Pu3s3cA1wdoXLFBt3vxV4Zp+7zwa+F13/HvC6US1UzNx9o7uviq63Ez70s0j4dgN40BHdzEcXB04Hro/uT9y2m9ls4B+Ab0a3jYRv8xAOel8f7wE/C3i63+110X1p0uzuG6Prm4DmShYmTmY2HzgBuJOUbHfUVLEa2AL8DngM2O7uvdFTkrjPfwn4V6AU3W4k+dtc5sBvzWylmV0Y3XfQ+3rFJ92WkePubmaJ7PdqZnXADcD73b2t/0z3Sd5udy8Ci81sEnAjcHSFixQrM3stsMXdV5rZSypdngo41d3Xm9k04Hdm9nD/Bw90Xx/vNXhN7A2bzWwGQLTcUuHyjDgzyxPC/Wp3/0l0d+K3uz933w4sB04BJplZuXKWtH3+RcBZZraW0OR6OnAFyd7mPu6+PlpuIXyhn8Qh7OvjPeA1sXfY3n+Krv8T8LMKlmXERe2v3wIecvcv9Hso0dsNYGZNUc0dM6sBXkE4BrEceGP0tERtu7t/xN1nu/t8wuf5Fnc/nwRvc5mZ1ZpZffk68Ergfg5hXx/3Z7Ka2WsIbXblib0/U+EixcbMfgy8hDCE6GbgfwM/Ba4D5hKGWn6Tu+97IHbcMrNTgduA+9jTJvtRQjt8YrcbwMyeSzioliVUxq5z90+b2UJC7XYKcDfwFnfvqlxJ4xE10fyLu782DdscbeON0c0c8CN3/4yZNXKQ+/q4D3gRERnYeG+iERGR/VDAi4gklAJeRCShFPAiIgmlgBcRSSgFvCSemRWj0fnKlxEbmMzM5vcf3VNkLNFQBZIGu919caULITLaVIOX1IrG3v5cNP7238zs8Oj++WZ2i5nda2Z/MLO50f3NZnZjND77PWb2wuitsmb2jWjM9t9GZ51iZu+LxrG/18yuqdBmSoop4CUNavZpolnW77Ed7r4I+CrhjGiArwDfc/fnAlcDX47u/zLwp2h89ucBD0T3HwF8zd2PBbYD50T3XwacEL3Pu+LaOJH90Zmsknhm1uHudQPcv5Ywocbj0YBmm9y90cxagRnu3hPdv9Hdp5pZCzC7/yny0RDGv4smY8DMPgzk3f1yM7sJ6CAMJ/HTfmO7i4wK1eAl7Xw/1w9E/zFRiuw5tvUPhBnHngfc1W80RJFRoYCXtFvWb3lHdP0vhJEMAc4nDHYGYbq0i6BvIo6J+3tTM8sAc9x9OfBhYCLwrF8RInFSjULSoCaaFansJncvd5WcbGb3Emrh50X3XQx8x8wuBVqAC6L7LwGuMrN3EGrqFwEbGVgW+GH0JWDAl6Mx3UVGjdrgJbWiNvgl7t5a6bKIxEFNNCIiCaUavIhIQqkGLyKSUAp4EZGEUsCLiCSUAl5EJKEU8CIiCfX/Ad0xW8jdbx4aAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K7cIh8Hz8_C",
        "outputId": "043ad766-8fcf-4dc2-f7be-f0a4c569c7c7"
      },
      "source": [
        "#loading weights from best model\n",
        "model.load_weights(\"mymodel_22\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbf7c8a0b50>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkA9J53-zwuL",
        "outputId": "25a78505-5d9f-4f05-db99-bf9d935e73b4"
      },
      "source": [
        "#test_data_prediction and submission file creation\n",
        "filename_features_test = 'test_features.csv'\n",
        "one_hot_encoded_seqs_array_test = clean_seq(filename_features_test)\n",
        "one_hot_encoded_seqs_array_test = clean_seq_reshape(one_hot_encoded_seqs_array_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750\n",
            "(8306, 3750)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-USQ0pPqCRSA",
        "outputId": "14fc7c5b-212b-486d-c22a-ab0b7ca80e60"
      },
      "source": [
        "#predicting on test data\n",
        "encoded_predict_labels = np.argmax(model.predict(one_hot_encoded_seqs_array_test), axis=1)\n",
        "print(encoded_predict_labels)\n",
        "test_predictions = decode_labels(encoded_predict_labels, le)\n",
        "print(test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 659    7  200 ...  658 1059  590]\n",
            "[ 667.    8.  205. ...  666. 1071.  597.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "vswkN-vMW3YU",
        "outputId": "4af74548-a163-4311-bf8f-0a58f0a0cbb5"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78f45ffb-f983-4c03-9065-56789d729697\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-78f45ffb-f983-4c03-9065-56789d729697\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving outliers_threshold_110_mahalanobis_distance_from_vae_embedding.csv to outliers_threshold_110_mahalanobis_distance_from_vae_embedding.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bOy_dWWXF6c",
        "outputId": "9de26386-8946-43ca-9af2-3ab25747bf2f"
      },
      "source": [
        "#Loading outlier predictions based on vae embedding and mahalanobis distance\n",
        "outlier_df = pd.read_csv('outliers_threshold_110_mahalanobis_distance_from_vae_embedding.csv')\n",
        "outlier_ids = outlier_df['labels']\n",
        "test_final_predictions = test_predictions\n",
        "\n",
        "for i in range(len(outlier_ids)):\n",
        "  if outlier_ids[i] > 0:\n",
        "    test_final_predictions[i] = -1 #overwriting outlier label predictions to -1\n",
        "\n",
        "print(len(test_final_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBegMscupBVN",
        "outputId": "6e5586d3-829e-4e06-cb9e-931297384c53"
      },
      "source": [
        "test_final_predictions = test_final_predictions.astype(int)\n",
        "print(test_final_predictions)\n",
        "test_ids = get_seq_ids(filename_features_test)\n",
        "test_ids = test_ids.astype(int)\n",
        "print(test_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 667    8  205 ...  666 1071  597]\n",
            "[   1    2    3 ... 8304 8305 8306]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LKRWYzsD208"
      },
      "source": [
        "def get_seq_ids(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    ids = df['id']\n",
        "    \n",
        "    return np.array(ids)\n",
        "\n",
        "frames = [pd.DataFrame(test_ids), pd.DataFrame(test_final_predictions)]\n",
        "output_data= np.concatenate(frames, axis=1)\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_csv('dna_barcode_seq_submission_cnn_mahalanobis_distance2.csv', index=False,  header=[\"id\",\"labels\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4EWtP2jtGQ24",
        "outputId": "1b54b13f-17c6-4712-93d9-9d1368345383"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"dna_barcode_seq_submission_cnn_mahalanobis_distance2.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1035b237-5bde-41aa-855d-eb529b4bcf14\", \"dna_barcode_seq_submission_cnn_mahalanobis_distance2.csv\", 74415)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}